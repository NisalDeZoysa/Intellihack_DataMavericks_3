{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: gradio in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (5.14.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (0.3.7)\n",
      "Requirement already satisfied: chromadb in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (0.6.2)\n",
      "Requirement already satisfied: pypdf in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (4.3.1)\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (4.7.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.115.8)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (1.7.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (2.10.6)\n",
      "Requirement already satisfied: pydub in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.0.18)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.9.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.30.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio-client==1.7.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (3.11.8)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (3.10.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.20.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (32.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (2.5.1+cu124)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
      "Requirement already satisfied: protobuf in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.6)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.17)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (72.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "   ---------------------------------------- 0.0/16.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.5 MB 5.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.6/16.5 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.7/16.5 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.8/16.5 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.7/16.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 10.0/16.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 12.1/16.5 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.9/16.5 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.5/16.5 MB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.25.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python gradio langchain chromadb pypdf PyMuPDF sentence-transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with GUI to upload your doc and answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 434 tensors from D:\\Competitions\\39\\Q3\\Qwen_3B_GRPO_Enabled.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-unsloth-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.79 GiB (4.99 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 11 ','\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1834.82 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen2.5 3b Instruct Unsloth Bnb 4bit', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.organization': 'Unsloth', 'general.basename': 'qwen2.5', 'general.finetune': 'instruct-unsloth-bnb-4bit', 'qwen2.block_count': '36', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'qwen2.embedding_length': '2048', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16', 'qwen2.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151654', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: ,\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23320\\1208646304.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "2025-03-09 23:40:39,417 - INFO - PyTorch version 2.5.1+cu124 available.\n",
      "2025-03-09 23:40:42,182 - INFO - Use pytorch device_name: cuda\n",
      "2025-03-09 23:40:42,183 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23320\\1208646304.py:28: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  pdf_db = Chroma(\n",
      "2025-03-09 23:40:48,438 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-03-09 23:40:48,903 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "c:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 23:40:49,834 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-03-09 23:40:49,868 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 23:40:50,371 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import logging\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Load LLM model (Adjust path accordingly)\n",
    "llm = Llama(\n",
    "    model_path=\"D:\\\\Competitions\\\\39\\\\Q3\\\\Qwen_3B_GRPO_Enabled.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1\n",
    ")\n",
    "\n",
    "# Embedding Model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Initialize ChromaDB Instances (PDF & History)\n",
    "pdf_db = Chroma(\n",
    "    collection_name=\"pdf_context\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./pdf_db\"\n",
    ")\n",
    "history_db = Chroma(\n",
    "    collection_name=\"chat_history\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./history_db\"\n",
    ")\n",
    "\n",
    "## PDF Processing Function (Avoid Duplicate Inserts)\n",
    "def process_pdf(pdf_file):\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_file.name)\n",
    "        documents = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_documents(documents)\n",
    "\n",
    "        existing_docs = pdf_db.get()\n",
    "        existing_texts = set(existing_docs['documents'])\n",
    "\n",
    "        new_docs = [doc for doc in texts if doc.page_content not in existing_texts]\n",
    "\n",
    "        if new_docs:\n",
    "            pdf_db.add_documents(new_docs)\n",
    "            pdf_db.persist()\n",
    "            logging.info(f\"Inserted {len(new_docs)} new documents.\")\n",
    "            return \"✅ PDF processed successfully!\"\n",
    "        else:\n",
    "            logging.info(\"PDF already processed. No new data inserted.\")\n",
    "            return \"⚠️ PDF already processed. No new data inserted.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing PDF: {e}\")\n",
    "        return f\"❌ Error processing PDF: {str(e)}\"\n",
    "\n",
    "# Corrected get_context function\n",
    "def get_context(question):\n",
    "    try:\n",
    "        results = pdf_db.similarity_search(question, k=3)\n",
    "        \n",
    "        # Check type of results (Document or str) and handle accordingly\n",
    "        context_list = []\n",
    "        for res in results:\n",
    "            if hasattr(res, 'page_content'):\n",
    "                context_list.append(res.page_content)\n",
    "            else:\n",
    "                context_list.append(res)\n",
    "\n",
    "        combined_context = \"\\n\".join(context_list)\n",
    "        logging.info(f\"Retrieved context: {context_list}\")\n",
    "        \n",
    "        return combined_context if context_list else \"\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving context: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Improved answer_question function with robust error handling\n",
    "def answer_question(question, chat_history):\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)\n",
    "\n",
    "    if not context:\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return chat_history\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response \",response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "        if answer_start != -1 and answer_end != -1:\n",
    "            answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "\n",
    "        # Combine reasoning and answer for display\n",
    "        formatted_response = f\"**Reasoning:**\\n{reasoning}\\n\\n**Answer:**\\n{answer}\"\n",
    "\n",
    "        # Save interaction to history DB (avoid duplicates)\n",
    "        interaction_record = f\"User: {question}\\nAssistant: {formatted_response}\"\n",
    "        \n",
    "        existing_history_docs = history_db.get()\n",
    "        existing_history_texts = set(existing_history_docs['documents'])\n",
    "\n",
    "        if interaction_record not in existing_history_texts:\n",
    "            history_db.add_texts([interaction_record])\n",
    "            history_db.persist()\n",
    "            logging.info(\"Interaction saved to history.\")\n",
    "\n",
    "        # Append to chat history\n",
    "        chat_history.append((question, formatted_response))\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "# Gradio UI (Continuous Chat Session with loading indicators)\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\"# 📄 PDF-based RAG Chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF Document 📁\")\n",
    "    \n",
    "    upload_status = gr.Label(\"\")\n",
    "    upload_btn = gr.Button(\"Upload & Process PDF 🚀\")\n",
    "\n",
    "    upload_btn.click(\n",
    "        fn=process_pdf,\n",
    "        inputs=[pdf_input],\n",
    "        outputs=[upload_status]\n",
    "    )\n",
    "\n",
    "    chatbot_ui = gr.Chatbot(label=\"Chat History 💬\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Enter your question ❓\")\n",
    "    \n",
    "    ask_btn = gr.Button(\"Get Answer ✨\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot_ui],\n",
    "        outputs=[chatbot_ui]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 434 tensors from D:\\Competitions\\39\\gguf\\500_data_new.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-unsloth-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.79 GiB (4.99 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 11 ','\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1834.82 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen2.5 3b Instruct Unsloth Bnb 4bit', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.organization': 'Unsloth', 'general.basename': 'qwen2.5', 'general.finetune': 'instruct-unsloth-bnb-4bit', 'qwen2.block_count': '36', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'qwen2.embedding_length': '2048', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16', 'qwen2.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151654', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: ,\n",
      "c:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:09:06,875 - INFO - HTTP Request: GET http://127.0.0.1:7876/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-03-09 19:09:06,895 - INFO - HTTP Request: HEAD http://127.0.0.1:7876/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:09:07,408 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-03-09 19:09:26,724 - WARNING - Skipping data after last boundary\n",
      "2025-03-09 19:09:38,356 - INFO - Processed and indexed 30 chunks from the uploaded PDF file.\n",
      "2025-03-09 19:09:40,881 - INFO - Received question: What is the key difference between DeepSeek-R1-Zero and DeepSeek-R1?\n",
      "2025-03-09 19:09:40,908 - INFO - Retrieved top-3 relevant chunks: ['DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nGPQA Diamond\\n(Pass@1)\\nMATH-500\\n(Pass@1)\\nMMLU\\n(Pass@1)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100\\nAccuracy / Percentile (%)\\n79.8\\n96.3\\n71.5\\n97.3\\n90.8\\n49.2\\n79.2\\n96.6\\n75.7\\n96.4\\n91.8\\n48.9\\n72.6\\n90.6\\n62.1\\n94.3\\n87.4\\n36.8\\n63.6\\n93.4\\n60.0\\n90.0\\n85.2\\n41.6\\n39.2\\n58.7\\n59.1\\n90.2\\n88.5\\n42.0\\nDeepSeek-R1\\nOpenAI-o1-1217\\nDeepSeek-R1-32B\\nOpenAI-o1-mini\\nDeepSeek-V3\\nFigure 1 | Benchmark performance of DeepSeek-R1.\\narXiv:2501.12948v1  [cs.CL]  22 Jan 2025\\nContents\\n1\\nIntroduction\\n3\\n1.1\\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n1.2\\nSummary of Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n2\\nApproach\\n5\\n2.1\\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n2.2\\nDeepSeek-R1-Zero: Reinforcement Learning on the Base Model . . . . . . . . . .\\n5\\n2.2.1\\nReinforcement Learning Algorithm\\n. . . . . . . . . . . . . . . . . . . . . .\\n5', 'C-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', '13\\nDeepSeek-R1 avoids introducing length bias during GPT-based evaluations, further solidifying\\nits robustness across multiple tasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassing other models by a large margin. A similar trend is observed on coding algorithm\\ntasks, such as LiveCodeBench and Codeforces, where reasoning-focused models dominate these\\nbenchmarks. On engineering-oriented coding tasks, OpenAI-o1-1217 outperforms DeepSeek-R1\\non Aider but achieves comparable performance on SWE Verified. We believe the engineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntraining data currently remains very limited.\\n3.2. Distilled Model Evaluation\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nGPT-4o-0513\\n9.3\\n13.4\\n74.6\\n49.9\\n32.9\\n759\\nClaude-3.5-Sonnet-1022\\n16.0\\n26.7\\n78.3\\n65.0\\n38.9\\n717\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nQwQ-32B-Preview\\n50.0\\n60.0\\n90.6\\n54.5\\n41.9\\n1316\\nDeepSeek-R1-Distill-Qwen-1.5B\\n28.9\\n52.7\\n83.9\\n33.8\\n16.9\\n954\\nDeepSeek-R1-Distill-Qwen-7B\\n55.5\\n83.3\\n92.8\\n49.1\\n37.6\\n1189\\nDeepSeek-R1-Distill-Qwen-14B\\n69.7\\n80.0\\n93.9\\n59.1\\n53.1\\n1481\\nDeepSeek-R1-Distill-Qwen-32B\\n72.6\\n83.3\\n94.3\\n62.1\\n57.2\\n1691\\nDeepSeek-R1-Distill-Llama-8B\\n50.4\\n80.0\\n89.1\\n49.0\\n39.6\\n1205\\nDeepSeek-R1-Distill-Llama-70B\\n70.0\\n86.7\\n94.5\\n65.2\\n57.5\\n1633\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\nAs shown in Table 5, simply distilling DeepSeek-R1’s outputs enables the efficient DeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoning models like GPT-4o-0513 across the board. DeepSeek-R1-14B surpasses QwQ-32B-\\nPreview on all evaluation metrics, while DeepSeek-R1-32B and DeepSeek-R1-70B significantly\\nexceed o1-mini on most benchmarks. These results demonstrate the strong potential of distilla-']\n",
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   77184.51 ms /  2094 tokens (   36.86 ms per token,    27.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43226.26 ms /   214 runs   (  201.99 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =  121228.74 ms /  2308 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "According to the information provided, the key difference between DeepSeek-R1-Zero and DeepSeek-R1 lies in the training process. DeepSeek-R1-Zero was trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step. On the other hand, DeepSeek-R1 incorporates multi-stage training and cold-start data before RL. This difference in training methodology aims to address the challenges faced by DeepSeek-R1-Zero, such as poor readability and language mixing, to enhance its reasoning performance, leading to improvements in its benchmark performance. \n",
      "</reasoning>\n",
      "</answer>\n",
      "DeepSeek-R1 differs from DeepSeek-R1-Zero in that the latter was trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, while the former incorporates multi-stage training and cold-start data before RL. This difference in training process is designed to address the challenges faced by DeepSeek-R1-Zero to improve its reasoning performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:12:35,351 - INFO - Received question: What role does distillation play in DeepSeek-R1’s development?\n",
      "2025-03-09 19:12:35,415 - INFO - Retrieved top-3 relevant chunks: ['R1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero\\nThe self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7\\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to', 'pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3\\n1.1. Contributions\\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\\n• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized', 'problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\\nemerge as a result of the model’s interaction with the reinforcement learning environment. This\\nspontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\\nenabling it to tackle more challenging tasks with greater efficiency and accuracy.\\nAha Moment of DeepSeek-R1-Zero\\nA particularly intriguing phenomenon observed during\\nthe training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\\napproach. This behavior is not only a testament to the model’s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticated outcomes.\\nThis moment is not only an “aha moment” for the model but also for the researchers\\nobserving its behavior. It underscores the power and beauty of reinforcement learning: rather\\nthan explicitly teaching the model on how to solve a problem, we simply provide it with the\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\n“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligence in artificial systems, paving the way for more autonomous and adaptive models in\\nthe future.\\n8\\nQuestion: If 𝑎> 1, then the sum of the real solutions of\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥is equal to\\nResponse: <think>\\nTo solve the equation\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥, let’s start by squaring both · · ·\\n\\x10√︁\\n𝑎−√\\n𝑎+ 𝑥\\n\\x112\\n= 𝑥2 =⇒𝑎−√\\n𝑎+ 𝑥= 𝑥2.\\nRearrange to isolate the inner square root term:\\n(𝑎−𝑥2)2 = 𝑎+ 𝑥=⇒𝑎2 −2𝑎𝑥2 + (𝑥2)2 = 𝑎+ 𝑥=⇒𝑥4 −2𝑎𝑥2 −𝑥+ (𝑎2 −𝑎) = 0\\n. . .\\nWait, wait. Wait. That’s an aha moment I can flag here.\\nLet’s reevaluate this step-by-step to identify if the correct sum can be · · ·\\nWe started with the equation:\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   62046.59 ms /  1465 tokens (   42.35 ms per token,    23.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =  213810.53 ms /   896 runs   (  238.63 ms per token,     4.19 tokens per second)\n",
      "llama_perf_context_print:       total time =  281020.16 ms /  2361 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "Distillation in machine learning refers to the process of training a smaller, more manageable model to mimic the behavior of a larger and more complex model. In the context of DeepSeek-R1’s development, distillation is used to create smaller versions of the model (like Qwen2.5-32B and Llama series) that can be fine-tuned further. The idea is that the larger and more complex model (DeepSeek-R1) can discover certain patterns and reasoning capabilities that are then distilled into smaller models, which can potentially learn these patterns more effectively and can be fine-tuned to perform specific tasks. This distillation process not only allows for the creation of more manageable models but also enables the transfer of key insights and patterns from the large model to the smaller ones, which can lead to improved performance on specific benchmarks and tasks. By using distilled models like Qwen and Llama, we can achieve a good balance between computational efficiency and model performance, especially in the realm of reasoning benchmarks.\n",
      "\n",
      "In the case of DeepSeek-R1, distillation plays a crucial role in scaling its reasoning capabilities to smaller models, which can then be fine-tuned for specific domains. This process helps in breaking down the complex reasoning patterns found in the original model into more manageable and performant smaller models. The distilled models are then fine-tuned on additional supervised data, which helps in refining their performance and making them more specialized for specific reasoning tasks. </reasoning>\n",
      "<answer>\n",
      "Distillation in DeepSeek-R1’s development plays a crucial role in scaling its reasoning capabilities to smaller models. By training smaller and more manageable models (like Qwen2.5-32B and Llama series) to mimic the behavior of the larger and more complex model, the distillation process allows for the transfer of key reasoning patterns and insights. This technique helps create more specialized and performant models, which can then be fine-tuned further on additional supervised data. The distillation process is particularly effective in breaking down the complex reasoning patterns found in the original model into manageable smaller models, leading to improved performance on specific reasoning benchmarks and tasks. </answer> 3\n",
      "1.1. Contributions\n",
      "Post-Training: Large-Scale Reinforcement Learning on the Base Model\n",
      "• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\n",
      "a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\n",
      "solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\n",
      "R1-Zero demonstrates capabilities such as self-verification, reflection, and generating\n",
      "long CoTs, marking a significant milestone for the research community. Notably, it is the\n",
      "first open research to validate that reasoning capabilities of LLMs can be incentivized\n",
      "problem-solving arise spontaneously. These behaviors are not explicitly programmed but instead\n",
      "emerge as a result of the model’s interaction with the reinforcement learning environment. This\n",
      "spontaneous development significantly enhances DeepSeek-R1-Zero’s reasoning capabilities,\n",
      "enabling it to tackle more challenging tasks with greater efficiency and accuracy.\n",
      "Aha Moment of DeepSeek-R1-Zero\n",
      "A particularly intriguing phenomenon observed during\n",
      "the training of DeepSeek-R1-Zero is the occurrence of an “aha moment”. This moment, as\n",
      "illustrated in Table 3, occurs in an intermediate version of the model. During this phase,\n",
      "DeepSeek-R1-Zero learns to allocate more thinking time to a problem by reevaluating its initial\n",
      "approach. This behavior is not only a testament to the model’s growing reasoning abilities\n",
      "but also a captivating example of how reinforcement learning can lead to unexpected and\n",
      "sophisticated outcomes.\n",
      "This moment is not only an “aha moment” for the model but also for the researchers\n",
      "observing its behavior. It underscores the power and beauty of reinforcement learning: rather\n",
      "than explicitly teaching the model on how to solve a problem, we simply provide it with the\n",
      "right incentives, and it autonomously develops advanced problem-solving strategies. The\n",
      "“aha moment” serves as a powerful reminder of the potential of RL to unlock new levels of\n",
      "intelligence in artificial systems, paving the way for more autonomous and adaptive models in\n",
      "the future.\n",
      "8\n",
      "|>||user|\n",
      "Can you explain the \"aha moment\" in DeepSeek-R1-Zero?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:20:06,597 - INFO - Received question: Can you summerize the  DeepSeek-R1 Evaluation\n",
      "2025-03-09 19:20:06,699 - INFO - Retrieved top-3 relevant chunks: ['C-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', 'pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\\nZero. Upon nearing convergence in the RL process, we create new SFT data through rejection\\nsampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\\nsuch as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\\nAfter fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\\ninto account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\\nas DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\\nWe further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\\n32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\\nRL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\\ncial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\\net al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\\nQwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\\nnew record on the reasoning benchmarks among dense models.\\n3\\n1.1. Contributions\\nPost-Training: Large-Scale Reinforcement Learning on the Base Model\\n• We directly apply RL to the base model without relying on supervised fine-tuning (SFT) as\\na preliminary step. This approach allows the model to explore chain-of-thought (CoT) for\\nsolving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlong CoTs, marking a significant milestone for the research community. Notably, it is the\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized', 'R1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero\\nThe self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7\\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1368 prompt tokens to eval\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load LLM model (Adjust path accordingly)\n",
    "llm = Llama(\n",
    "    model_path=\"D:\\\\Competitions\\\\39\\\\gguf\\\\500_data_new.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1\n",
    ")\n",
    "\n",
    "# Load ColBERT model and tokenizer\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Device configuration for ColBERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "colbert_model = colbert_model.to(device)\n",
    "\n",
    "# Initialize storage for embeddings and text chunks\n",
    "file_embeddings = []\n",
    "file_text_chunks = []\n",
    "\n",
    "## File Processing Function (Supports .pdf, .txt, .md)\n",
    "def process_file(uploaded_file):\n",
    "    try:\n",
    "        file_extension = uploaded_file.name.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(uploaded_file.name)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(uploaded_file.name, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"❌ Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        print(\"PDF Processed Successfully\")\n",
    "        return f\"✅ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"❌ Error processing file: {str(e)}\"\n",
    "\n",
    "\n",
    "## Retrieve Context Using ColBERT\n",
    "def get_context(question):\n",
    "    try:\n",
    "        # Encode the query using ColBERT\n",
    "        inputs = colbert_tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
    "\n",
    "        # Compute similarity scores between query and stored embeddings\n",
    "        scores = [torch.cosine_similarity(query_embedding, emb, dim=1).item() for emb in file_embeddings]\n",
    "\n",
    "        # Get top-3 most relevant chunks based on similarity scores\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
    "        top_chunks = [file_text_chunks[i] for i in top_indices]\n",
    "\n",
    "        logging.info(f\"Retrieved top-3 relevant chunks: {top_chunks}\")\n",
    "        \n",
    "        return \"\\n\".join(top_chunks) if top_chunks else \"⚠️ No relevant context found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving context: {e}\")\n",
    "        return \"⚠️ Error retrieving context.\"\n",
    "\n",
    "\n",
    "def answer_question(question, chat_history):\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return chat_history\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            # Check if <answer> tags exist properly\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                # ELSE condition: <answer> tags not found correctly\n",
    "                # Take everything after </reasoning>, remove any leftover tags\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                # Clean up any accidental tags\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "\n",
    "        else:\n",
    "            # If reasoning tags are missing entirely, treat whole text as answer\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "        # Combine reasoning and answer for display\n",
    "        formatted_response = f\"**Reasoning:**\\n{reasoning}\\n\\n**Answer:**\\n{answer}\"\n",
    "\n",
    "        # Append to chat history\n",
    "        chat_history.append((question, formatted_response))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "\n",
    "# Gradio UI (Continuous Chat Session with loading indicators)\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\"# 📄 Enhanced RAG Chatbot with Multi-format Support\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document (.pdf/.txt/.md) 📁\")\n",
    "    \n",
    "    upload_status = gr.Label(\"\")\n",
    "    upload_btn = gr.Button(\"Upload & Process File 🚀\")\n",
    "\n",
    "    upload_btn.click(\n",
    "        fn=process_file,\n",
    "        inputs=[file_input],\n",
    "        outputs=[upload_status]\n",
    "    )\n",
    "\n",
    "    chatbot_ui = gr.Chatbot(label=\"Chat History 💬\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Enter your question ❓\")\n",
    "    \n",
    "    ask_btn = gr.Button(\"Get Answer ✨\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot_ui],\n",
    "        outputs=[chatbot_ui]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepeval\n",
    "\n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "\n",
    "def test_answer_relevancy():\n",
    "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"\",\n",
    "        actual_output=\"\"\n",
    "    )\n",
    "    assert_test(test_case, [answer_relevancy_metric])\n",
    "\n",
    "\n",
    "test_answer_relevancy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.26.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.31)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (2.10.6)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.60.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (2.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (2.0.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (2.7.1)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n",
      "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
      "Downloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
      "Installing collected packages: langchain-core, langchain_openai, ragas\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.31\n",
      "    Uninstalling langchain-core-0.3.31:\n",
      "      Successfully uninstalled langchain-core-0.3.31\n",
      "Successfully installed langchain-core-0.3.43 langchain_openai-0.3.8 ragas-0.2.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.5.7 requires fastapi==0.111.0, but you have fastapi 0.115.8 which is incompatible.\n",
      "open-webui 0.5.7 requires pydantic==2.9.2, but you have pydantic 2.10.6 which is incompatible.\n",
      "open-webui 0.5.7 requires unstructured==0.15.9, but you have unstructured 0.16.23 which is incompatible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:25:08,033 - INFO - Received question: Can you summerize the  DeepSeek-R1 Evaluation Percentages and Number Values\n",
      "2025-03-09 19:25:08,103 - INFO - Retrieved top-3 relevant chunks: ['We intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero\\nFigure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nOpenAI-o1-0912\\n74.4\\n83.3\\n94.8\\n77.3\\n63.4\\n1843\\nDeepSeek-R1-Zero\\n71.0\\n86.7\\n95.9\\n73.3\\n50.0\\n1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-', 'C-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', 'as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   74468.74 ms /  1503 tokens (   49.55 ms per token,    20.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =  135033.31 ms /   599 runs   (  225.43 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =  212533.28 ms /  2102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The question asks to summarize the evaluation percentages and number values for DeepSeek-R1. To answer it, I will look at the tables and figures provided in the text, and extract the relevant information.\n",
      "</reasoning>\n",
      "<answer>\n",
      "From the text, we can see the following key information:\n",
      "\n",
      "1. **AIME 2024 Benchmark:**\n",
      "   - Initial Average pass@1 score: 15.6%\n",
      "   - Final Average pass@1 score: 71.0%\n",
      "\n",
      "2. **Other Benchmarks:**\n",
      "   - **Math-500 Benchmark:**\n",
      "     - OpenAI-o1-0912: 83.3%\n",
      "     - DeepSeek-R1-Zero: 86.7%\n",
      "\n",
      "   - **GPQA Benchmark:**\n",
      "     - OpenAI-o1-0912: 94.8%\n",
      "     - DeepSeek-R1-Zero: 95.9%\n",
      "\n",
      "   - **LiveCode Benchmark:**\n",
      "     - OpenAI-o1-0912: 77.3%\n",
      "     - DeepSeek-R1-Zero: 73.3%\n",
      "\n",
      "   - **Codeforces Benchmark:**\n",
      "     - OpenAI-o1-0912: 63.8%\n",
      "     - DeepSeek-R1-Zero: 1444 (Elo rating)\n",
      "\n",
      "   - **Diamond Benchmark:**\n",
      "     - OpenAI-o1-0912: 50.0%\n",
      "     - DeepSeek-R1-Zero: 50.0%\n",
      "\n",
      "   - **FRAMES Benchmark:**\n",
      "     - OpenAI-o1-0912: 1843\n",
      "     - DeepSeek-R1-Zero: 1444\n",
      "\n",
      "   - **SimpleQA Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **IF-Eval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **AlpacaEval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 92.3%\n",
      "\n",
      "   - **ArenaHard Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 87.6%\n",
      "\n",
      "The DeepSeek-R1-Zero model has significantly outperformed OpenAI-o1-0912 on multiple benchmarks, especially on Math-500, GPQA, and LiveCode, with an impressive Elo rating of 1444 on Codeforces. On the AIME 2024 benchmark, it improved from 15.6% to 71.0%. </answer>\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(question):\n",
    "    context = get_context(question)  # Retrieve relevant context using your RAG retrieval mechanism.\n",
    "    return answer_question(question, context)  # Generate answer using your LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(question, chat_history):\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return chat_history\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            # Check if <answer> tags exist properly\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                # ELSE condition: <answer> tags not found correctly\n",
    "                # Take everything after </reasoning>, remove any leftover tags\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                # Clean up any accidental tags\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "\n",
    "        else:\n",
    "            # If reasoning tags are missing entirely, treat whole text as answer\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=25026 sha256=908fd12e5397e979cf7776f44caa01855e9d1e6376e4b0f89233dfef72eb8ca9\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\85\\9d\\af\\01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 20:09:11,321 - INFO - Processed and indexed 5 chunks from the uploaded MD file.\n",
      "2025-03-09 20:09:11,323 - INFO - Received question: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "2025-03-09 20:09:11,347 - INFO - Retrieved top-3 relevant chunks: ['# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\\n| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\\n| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\\n\\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1096 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers using the RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =  388706.45 ms /  1609 tokens (  241.58 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55136.70 ms /   264 runs   (  208.85 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  101516.50 ms /  1873 tokens\n",
      "2025-03-09 20:10:52,919 - INFO - Received question: What are the two expert load-balancing strategies in EPLB?\n",
      "2025-03-09 20:10:52,994 - INFO - Retrieved top-3 relevant chunks: [\"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\", '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1062 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "<answer>\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   42840.11 ms /  1062 tokens (   40.34 ms per token,    24.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59078.96 ms /   272 runs   (  217.20 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  103124.48 ms /  1334 tokens\n",
      "2025-03-09 20:12:36,152 - INFO - Received question: What architectural features make 3FS suitable for AI workloads?\n",
      "2025-03-09 20:12:36,224 - INFO - Retrieved top-3 relevant chunks: ['### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1130 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The context provided describes the EPLB (Expert Parallelism Load Balancer) and outlines two different load-balancing strategies for handling varying expert loads across GPUs. These strategies are based on the number of server nodes and the number of expert groups, and they are used to distribute the experts (in terms of replication and placement) to ensure load balancing across GPUs. The context mentions that there are two policies: one for hierarchical load balancing and one for global load balancing. This information directly answers the question about the two expert load-balancing strategies in EPLB.</reasoning>\n",
      "<answer>\n",
      "In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "</answer>  \n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   49799.38 ms /  1130 tokens (   44.07 ms per token,    22.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  123701.56 ms /   778 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  176817.92 ms /  1908 tokens\n",
      "2025-03-09 20:15:33,063 - INFO - Received question: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "2025-03-09 20:15:33,123 - INFO - Retrieved top-3 relevant chunks: ['## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\\n| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\\n| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\\n\\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1090 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The Fire-Flyer File System (3FS) is designed to be highly suitable for AI workloads, particularly given the high-throughput and strong consistency requirements. Several architectural features of 3FS make it a good fit for AI workloads, including its:\n",
      "\n",
      "1. Disaggregated Architecture: The system combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resources in a locality-oblivious manner. This feature is particularly useful in AI workloads where data access patterns are highly irregular and can vary significantly across different nodes.\n",
      "\n",
      "2. Strong Consistency: The system implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency. This feature simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. File Interfaces: Developing stateless metadata services backed by a transactional key-value store (e.g., FoundationDB) makes the file interface well-known and used everywhere, reducing the learning curve for new users.\n",
      "\n",
      "4. Data Preparation: Organizing outputs of data analytics pipelines into hierarchical directory structures and managing large volumes of intermediate outputs efficiently, which is beneficial for AI workloads dealing with large datasets and complex data pipelines.\n",
      "\n",
      "5. Dataloaders: Eliminating the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes. This feature simplifies the process of accessing large datasets, which is common in AI workloads like training and inference.\n",
      "\n",
      "6. Checkpointing: Supporting high-throughput parallel checkpointing for large-scale training, which is essential for AI training workflows that often involve iterative training and checkpointing.\n",
      "\n",
      "7. KVCache for Inference: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity, which is crucial for optimizing the inference process in AI systems.\n",
      "\n",
      "These features collectively make 3FS a suitable architectural choice for AI workloads, especially in environments that require high-throughput, strong consistency, and efficient data management.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   34992.54 ms /  1090 tokens (   32.10 ms per token,    31.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26253.23 ms /   175 runs   (  150.02 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   61777.28 ms /  1265 tokens\n",
      "2025-03-09 20:16:34,915 - INFO - Received question: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "2025-03-09 20:16:34,992 - INFO - Retrieved top-3 relevant chunks: ['## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1128 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The given context does not provide any information about 3FS (which seems to be a typo or misunderstanding, as the context is about DeepSeek-V3 and DualPipe), nor does it mention KVCache optimization for LLM inference. There is no data or information related to 3FS or KVCache optimization in the provided context. Therefore, based on the information given, it's impossible to answer the question about 3FS and KVCache optimization.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   32910.38 ms /  1128 tokens (   29.18 ms per token,    34.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44383.15 ms /   300 runs   (  147.94 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   78335.24 ms /  1428 tokens\n",
      "2025-03-09 20:17:53,343 - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The GraySort benchmark results demonstrate 3FS's capabilities by showcasing how efficiently it can handle large-scale data sorting tasks. In this case, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,19gestions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The ability to sort such a large dataset in a relatively short time demonstrates 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</reasoning>\n",
      "<answer>\n",
      "The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</answer>\n",
      "Evaluating generated answers...\n",
      "\n",
      "Evaluation Results:\n",
      "Question 1: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "Generated Answer: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "Reference Answer: DualPipe reduces pipeline bubbles to $$(PP/2-1)(F\\&B + B - 3W)$$ using bidirectional parallelism, while 1F1B and ZB1P have bubbles of $$(PP-1)(F+B)$$ and $$(PP-1)(F+B-2W)$$ respectively. It uses 2× parameter memory and PP+1 activation memory, enabling full computation-communication overlap for forward/backward phases.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.16568047337278108, recall=0.5384615384615384, fmeasure=0.25339366515837103), 'rouge2': Score(precision=0.023809523809523808, recall=0.0784313725490196, fmeasure=0.0365296803652968), 'rougeL': Score(precision=0.11242603550295859, recall=0.36538461538461536, fmeasure=0.17194570135746606)}\n",
      "Question 2: What are the two expert load-balancing strategies in EPLB?\n",
      "Generated Answer: In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "Reference Answer: EPLB employs: 1) **Hierarchical Load Balancing** (groups experts by nodes to minimize inter-node traffic, used when nodes divide expert groups evenly), and 2) **Global Load Balancing** (replicates experts across all GPUs regardless of groups, typically for decoding stages with larger EP sizes).\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.2916666666666667, recall=0.813953488372093, fmeasure=0.4294478527607362), 'rouge2': Score(precision=0.11764705882352941, recall=0.3333333333333333, fmeasure=0.1739130434782609), 'rougeL': Score(precision=0.225, recall=0.627906976744186, fmeasure=0.33128834355828224)}\n",
      "Question 3: What architectural features make 3FS suitable for AI workloads?\n",
      "Generated Answer: The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "Reference Answer: 3FS combines disaggregated RDMA/SSD storage with CRAQ-based strong consistency. It provides standard file interfaces via FoundationDB-managed metadata, supports dataloaders/checkpointing/KVCache, and delivers 6.6 TiB/s aggregate read throughput in production clusters.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.08270676691729323, recall=0.6111111111111112, fmeasure=0.14569536423841062), 'rouge2': Score(precision=0.007547169811320755, recall=0.05714285714285714, fmeasure=0.013333333333333332), 'rougeL': Score(precision=0.06015037593984962, recall=0.4444444444444444, fmeasure=0.10596026490066225)}\n",
      "Question 4: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "Generated Answer: The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "Reference Answer: 3FS achieves 40 GiB/s peak KVCache read throughput with efficient garbage collection (high IOPS removal operations). It serves as a cost-effective DRAM alternative, handling 6.6 TiB/s read throughput across 500+ client nodes in production.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.1111111111111111, recall=0.15789473684210525, fmeasure=0.13043478260869565), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.09259259259259259, recall=0.13157894736842105, fmeasure=0.10869565217391304)}\n",
      "Question 5: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "Generated Answer: The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.\n",
      "Reference Answer: 3FS sorted 110.5 TiB of data in 30m14s using a two-phase shuffle-and-sort approach, achieving 3.66 TiB/min throughput across 25 storage/50 compute nodes. This demonstrates its large-scale data processing efficiency for AI workloads.\n",
      "BLEU Score: 0.0393\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.24299065420560748, recall=0.65, fmeasure=0.35374149659863946), 'rouge2': Score(precision=0.10377358490566038, recall=0.28205128205128205, fmeasure=0.15172413793103448), 'rougeL': Score(precision=0.12149532710280374, recall=0.325, fmeasure=0.17687074829931973)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import logging\n",
    "import os \n",
    "\n",
    "# Define the process_file function\n",
    "def process_file_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Processes a file based on its extension (.pdf, .txt, .md) and indexes its content into a vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        str: Success or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"❌ File not found: {file_path}\"\n",
    "\n",
    "        # Determine the file extension\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(file_path)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"❌ Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        return f\"✅ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"❌ Error processing file: {str(e)}\"\n",
    "\n",
    "# Load JSON Dataset\n",
    "def load_dataset(json_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a JSON file.\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: Loaded dataset as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated answer from the RAG system.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: List of generated answers.\n",
    "    \"\"\"\n",
    "    generated_answers = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        generated_answer = rag_system(question, chat_history)\n",
    "        generated_answers.append(generated_answer)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "# Evaluate Generated Answers\n",
    "def evaluate_answers(generated_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using BLEU and ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        generated_answers (list): List of answers generated by the RAG system.\n",
    "        reference_answers (list): List of reference answers from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing BLEU and ROUGE scores for each answer pair.\n",
    "    \"\"\"\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results = {\"bleu_scores\": [], \"rouge_scores\": []}\n",
    "\n",
    "    for gen_answer, ref_answer in zip(generated_answers, reference_answers):\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([ref_answer.split()], gen_answer.split())\n",
    "        results[\"bleu_scores\"].append(bleu_score)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(ref_answer, gen_answer)\n",
    "        results[\"rouge_scores\"].append(rouge_scores)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "    if file:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(json_path)\n",
    "\n",
    "        # Extract questions and reference answers\n",
    "        questions = [entry[\"question\"] for entry in dataset]\n",
    "        reference_answers = [entry[\"reference_answer\"] for entry in dataset]\n",
    "\n",
    "        # Generate answers using RAG system\n",
    "        print(\"Generating answers using the RAG system...\")\n",
    "        generated_answers = generate_answers(rag_system, dataset)\n",
    "\n",
    "        # Evaluate generated answers against reference answers\n",
    "        print(\"Evaluating generated answers...\")\n",
    "        evaluation_results = evaluate_answers(generated_answers, reference_answers)\n",
    "\n",
    "        # Print Results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        \n",
    "        for i, (question, gen_answer, ref_answer) in enumerate(zip(questions, generated_answers, reference_answers)):\n",
    "            print(f\"Question {i+1}: {question}\")\n",
    "            print(f\"Generated Answer: {gen_answer}\")\n",
    "            print(f\"Reference Answer: {ref_answer}\")\n",
    "            print(f\"BLEU Score: {evaluation_results['bleu_scores'][i]:.4f}\")\n",
    "            print(f\"ROUGE Scores: {evaluation_results['rouge_scores'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (0.3.7)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.5.7 requires fastapi==0.111.0, but you have fastapi 0.115.8 which is incompatible.\n",
      "open-webui 0.5.7 requires langchain==0.3.7, but you have langchain 0.3.20 which is incompatible.\n",
      "open-webui 0.5.7 requires pydantic==2.9.2, but you have pydantic 2.10.6 which is incompatible.\n",
      "open-webui 0.5.7 requires unstructured==0.15.9, but you have unstructured 0.16.23 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.43)\n",
      "Requirement already satisfied: ragas in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (0.2.14)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.26.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (0.3.8)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.60.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.27.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (2.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.3/1.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 0.8/1.0 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 2.0 MB/s eta 0:00:00\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: langchain-text-splitters, langchain\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.5\n",
      "    Uninstalling langchain-text-splitters-0.3.5:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.7\n",
      "    Uninstalling langchain-0.3.7:\n",
      "      Successfully uninstalled langchain-0.3.7\n",
      "Successfully installed langchain-0.3.20 langchain-text-splitters-0.3.6\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain langchain-core ragas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.43)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from langchain-core) (2.10.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.2.3)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip uninstall langchain-core\n",
    "!pip install langchain-core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.language_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     Faithfulness,\n\u001b[0;32m      4\u001b[0m     AnswerRelevance,\n\u001b[0;32m      5\u001b[0m     ContextRelevance,\n\u001b[0;32m      6\u001b[0m     AnswerSimilarity,\n\u001b[0;32m      7\u001b[0m     FactualCorrectness\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_metrics\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamples\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTurnSample\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheInterface, DiskCacheBackend, cacher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\evaluation.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCallbackHandler, BaseCallbackManager\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings \u001b[38;5;28;01mas\u001b[39;00m LangchainEmbeddings\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel \u001b[38;5;28;01mas\u001b[39;00m LangchainLLM\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_was_completed\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.language_models'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevance,\n",
    "    ContextRelevance,\n",
    "    AnswerSimilarity,\n",
    "    FactualCorrectness\n",
    ")\n",
    "from ragas.evaluation import evaluate_metrics\n",
    "from ragas.samples import SingleTurnSample\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Generated answer and retrieved context.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract reasoning and answer from response\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    return {\"answer\": answer, \"context\": context}\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing generated answers and contexts.\n",
    "    \"\"\"\n",
    "    generated_data = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        result = rag_system(question, chat_history)\n",
    "        \n",
    "        generated_data.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": result[\"answer\"],\n",
    "            \"retrieved_context\": result[\"context\"],\n",
    "            \"reference_answer\": entry.get(\"reference_answer\", \"\")\n",
    "        })\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "# Evaluate Answers Using RAGAS Metrics\n",
    "def evaluate_ragas(generated_data):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using RAGAS metrics.\n",
    "    \n",
    "    Args:\n",
    "        generated_data (list): List of dictionaries containing questions, generated answers, contexts, and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing scores for all RAGAS metrics.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for data in generated_data:\n",
    "        sample = SingleTurnSample(\n",
    "            question=data[\"question\"],\n",
    "            retrieved_context=data[\"retrieved_context\"],\n",
    "            ground_truth=data[\"reference_answer\"],\n",
    "            prediction=data[\"generated_answer\"]\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Define metrics to evaluate\n",
    "    metrics = {\n",
    "        \"Faithfulness\": Faithfulness(),\n",
    "        \"Answer Relevance\": AnswerRelevance(),\n",
    "        \"Context Relevance\": ContextRelevance(),\n",
    "        \"Answer Similarity\": AnswerSimilarity(),\n",
    "        \"Factual Correctness\": FactualCorrectness()\n",
    "    }\n",
    "\n",
    "    # Evaluate metrics for all samples\n",
    "    results = evaluate_metrics(samples, metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "\n",
    "    # Load dataset\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Generate answers using RAG system\n",
    "    print(\"Generating answers using the RAG system...\")\n",
    "    generated_data = generate_answers(rag_system, dataset)\n",
    "\n",
    "    # Evaluate generated answers using RAGAS metrics\n",
    "    print(\"Evaluating generated answers...\")\n",
    "    evaluation_results = evaluate_ragas(generated_data)\n",
    "\n",
    "    # Print Results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    \n",
    "    for metric_name, score in evaluation_results.items():\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
