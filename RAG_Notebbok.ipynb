{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting PyMuPDF\n",
      "  Using cached pymupdf-1.25.3-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from llama-cpp-python) (4.12.2)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.3.0 (from gradio)\n",
      "  Downloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting matplotlib~=3.0 (from gradio)\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.15-cp39-cp39-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from gradio) (24.2)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<11.0,>=8.0 (from gradio)\n",
      "  Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.9.10-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting urllib3~=2.0 (from gradio)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec (from gradio-client==1.3.0->gradio)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio)\n",
      "  Downloading websockets-12.0-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.41 (from langchain)\n",
      "  Using cached langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.3.13-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.38-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp39-cp39-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.19.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tqdm>=4.65.0 (from chromadb)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.70.0-cp39-cp39-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.1.0-cp39-cp39-win_amd64.whl.metadata (16 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Collecting filelock (from huggingface_hub)\n",
      "  Using cached filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Collecting idna>=2.8 (from anyio<5.0,>=3.0->gradio)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from build>=1.0.3->chromadb) (8.6.1)\n",
      "Collecting tomli>=1.1.0 (from build>=1.0.3->chromadb)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting certifi (from httpx>=0.24.1->gradio)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.41->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp39-cp39-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio)\n",
      "  Downloading fonttools-4.56.0-cp39-cp39-win_amd64.whl.metadata (103 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-6.30.0-cp39-cp39-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata>=4.6 (from build>=1.0.3->chromadb)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.29.3-cp39-cp39-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro>=1.5.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl.metadata (36 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\rag_env\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.1.1-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-1.0.4-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\n",
      "   ---------------------------------------- 0.0/18.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.0/18.1 MB 7.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.9/18.1 MB 8.0 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 4.7/18.1 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 7.1/18.1 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 8.7/18.1 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.7/18.1 MB 8.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 12.6/18.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.4/18.1 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.3/18.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  17.8/18.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.1/18.1 MB 7.9 MB/s eta 0:00:00\n",
      "Downloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\n",
      "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
      "Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Downloading chroma_hnswlib-0.7.6-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
      "Using cached pymupdf-1.25.3-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading grpcio-1.70.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 2.1/4.3 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 8.1 MB/s eta 0:00:00\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.5 MB/s eta 0:00:00\n",
      "Using cached langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
      "Using cached langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Downloading langsmith-0.3.13-py3-none-any.whl (339 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.1/7.8 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.9/7.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.0/7.8 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.6/7.8 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading mmh3-5.1.0-cp39-cp39-win_amd64.whl (41 kB)\n",
      "Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "   ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/15.9 MB 9.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 3.4/15.9 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.5/15.9 MB 8.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.6/15.9 MB 9.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.2/15.9 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.0/15.9 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.1/15.9 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.9 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.9/15.9 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading onnxruntime-1.19.2-cp39-cp39-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.1 MB 9.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/11.1 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 10.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.0/11.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 8.2 MB/s eta 0:00:00\n",
      "Using cached opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
      "Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
      "Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "Downloading orjson-3.10.15-cp39-cp39-win_amd64.whl (133 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.3/11.6 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading pillow-10.4.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.3/2.6 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.4/2.6 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading posthog-3.19.0-py2.py3-none-any.whl (77 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.8/2.0 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.3 MB/s eta 0:00:00\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-win_amd64.whl (162 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading ruff-0.9.10-py3-none-win_amd64.whl (11.4 MB)\n",
      "   ---------------------------------------- 0.0/11.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.4 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/11.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.1/11.4 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.9/11.4 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.4 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.4 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.4/11.4 MB 7.6 MB/s eta 0:00:00\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading SQLAlchemy-2.0.38-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 6.9 MB/s eta 0:00:00\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/204.1 MB 10.7 MB/s eta 0:00:19\n",
      "    --------------------------------------- 3.9/204.1 MB 10.2 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 6.0/204.1 MB 10.0 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 7.9/204.1 MB 9.7 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 10.0/204.1 MB 9.7 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 11.8/204.1 MB 9.8 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 13.6/204.1 MB 9.6 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 15.5/204.1 MB 9.5 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 17.8/204.1 MB 9.7 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 19.9/204.1 MB 9.8 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 21.5/204.1 MB 9.6 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 22.8/204.1 MB 9.3 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 24.9/204.1 MB 9.4 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 27.0/204.1 MB 9.4 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 29.1/204.1 MB 9.4 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 31.2/204.1 MB 9.5 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 33.3/204.1 MB 9.5 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 35.1/204.1 MB 9.5 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 37.2/204.1 MB 9.5 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 38.5/204.1 MB 9.4 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 40.4/204.1 MB 9.4 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 42.2/204.1 MB 9.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 44.0/204.1 MB 9.3 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 45.9/204.1 MB 9.3 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 47.7/204.1 MB 9.2 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 49.5/204.1 MB 9.2 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 51.6/204.1 MB 9.3 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 54.0/204.1 MB 9.3 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 56.1/204.1 MB 9.4 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 57.4/204.1 MB 9.3 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 59.2/204.1 MB 9.3 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 61.1/204.1 MB 9.3 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 62.9/204.1 MB 9.3 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 64.7/204.1 MB 9.3 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 66.8/204.1 MB 9.3 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 69.2/204.1 MB 9.3 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 71.0/204.1 MB 9.4 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 73.1/204.1 MB 9.4 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 75.0/204.1 MB 9.4 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 77.6/204.1 MB 9.4 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 79.7/204.1 MB 9.4 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 81.8/204.1 MB 9.5 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 84.1/204.1 MB 9.5 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 86.2/204.1 MB 9.5 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 88.3/204.1 MB 9.6 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 90.2/204.1 MB 9.5 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 92.5/204.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 94.4/204.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 96.2/204.1 MB 9.5 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 98.3/204.1 MB 9.6 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 100.7/204.1 MB 9.6 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 102.5/204.1 MB 9.6 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 104.6/204.1 MB 9.6 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 107.0/204.1 MB 9.6 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 108.8/204.1 MB 9.6 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 110.6/204.1 MB 9.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 112.7/204.1 MB 9.6 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 114.8/204.1 MB 9.7 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 116.9/204.1 MB 9.7 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 118.8/204.1 MB 9.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 120.8/204.1 MB 9.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 122.9/204.1 MB 9.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 124.8/204.1 MB 9.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 126.9/204.1 MB 9.7 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 128.7/204.1 MB 9.6 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 130.8/204.1 MB 9.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 132.9/204.1 MB 9.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 135.0/204.1 MB 9.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 137.4/204.1 MB 9.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 138.7/204.1 MB 9.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 140.8/204.1 MB 9.7 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 142.6/204.1 MB 9.6 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 144.7/204.1 MB 9.7 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 146.8/204.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 148.9/204.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 151.3/204.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 153.6/204.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 155.5/204.1 MB 9.7 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 157.8/204.1 MB 9.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 159.9/204.1 MB 9.7 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 162.3/204.1 MB 9.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 163.8/204.1 MB 9.7 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 165.4/204.1 MB 9.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 167.5/204.1 MB 9.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 169.3/204.1 MB 9.7 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 171.4/204.1 MB 9.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 173.5/204.1 MB 9.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 175.9/204.1 MB 9.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 178.3/204.1 MB 9.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 180.1/204.1 MB 9.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 182.2/204.1 MB 9.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 184.3/204.1 MB 9.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 186.1/204.1 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 188.2/204.1 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 190.3/204.1 MB 9.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 192.4/204.1 MB 9.8 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.5/204.1 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 196.6/204.1 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 198.4/204.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  200.0/204.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.1/204.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 204.1/204.1 MB 9.6 MB/s eta 0:00:00\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Using cached filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.2 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.2/11.2 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.2 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.7/11.2 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 9.1 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/46.2 MB 9.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 3.7/46.2 MB 9.9 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 10.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 7.9/46.2 MB 9.9 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 10.0/46.2 MB 10.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 12.3/46.2 MB 10.3 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 14.2/46.2 MB 10.1 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 16.0/46.2 MB 10.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 18.1/46.2 MB 10.0 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 19.9/46.2 MB 9.9 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 21.5/46.2 MB 9.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 23.6/46.2 MB 9.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 25.7/46.2 MB 9.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.8/46.2 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 29.4/46.2 MB 9.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 31.2/46.2 MB 9.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 33.0/46.2 MB 9.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.9/46.2 MB 9.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.7/46.2 MB 9.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 38.5/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 40.6/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.7/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 44.6/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 9.3 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading fonttools-4.56.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -------------------------------------- - 2.1/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 9.5 MB/s eta 0:00:00\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading googleapis_common_protos-1.69.1-py2.py3-none-any.whl (293 kB)\n",
      "Downloading greenlet-3.1.1-cp39-cp39-win_amd64.whl (298 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.4-cp39-cp39-win_amd64.whl (89 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.3-cp39-cp39-win_amd64.whl (434 kB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading watchfiles-1.0.4-cp39-cp39-win_amd64.whl (284 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-12.0-cp39-cp39-win_amd64.whl (124 kB)\n",
      "Downloading zstandard-0.23.0-cp39-cp39-win_amd64.whl (495 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 9.8 MB/s eta 0:00:00\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: llama-cpp-python, pypika\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): started\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): still running...\n",
      "  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp39-cp39-win_amd64.whl size=3782025 sha256=8060e8ad8d131aa11ad009ac9f1116b1b3de58c40027332ecd90a9e0c135c3a1\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\db\\ea\\a9\\1655836e014901e76aa17b228e560e8c3bf83ed00749aa0ed2\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53882 sha256=5ecdb762d60d327bb25e0cf1f3a5d3e008438ff12772a48fe92bdec7b7cd58c7\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\f7\\02\\64\\d541eac67ec459309d1fb19e727f58ecf7ffb4a8bf42d4cfe5\n",
      "Successfully built llama-cpp-python pypika\n",
      "Installing collected packages: pytz, pypika, pydub, mpmath, monotonic, flatbuffers, durationpy, zstandard, wrapt, websockets, websocket-client, urllib3, tzdata, tqdm, tomlkit, tomli, threadpoolctl, tenacity, sympy, sniffio, shellingham, semantic-version, safetensors, ruff, regex, pyyaml, python-multipart, python-dotenv, pyreadline3, pyproject_hooks, pypdf, pyparsing, PyMuPDF, pydantic-core, pyasn1, protobuf, pillow, overrides, orjson, opentelemetry-util-http, oauthlib, numpy, networkx, mmh3, mdurl, markupsafe, kiwisolver, jsonpointer, joblib, importlib-resources, importlib-metadata, idna, httptools, h11, grpcio, greenlet, fsspec, fonttools, filelock, ffmpy, distro, diskcache, cycler, click, charset-normalizer, certifi, cachetools, bcrypt, backoff, async-timeout, asgiref, annotated-types, aiofiles, uvicorn, SQLAlchemy, scipy, rsa, requests, pydantic, pyasn1-modules, pandas, opentelemetry-proto, markdown-it-py, jsonpatch, jinja2, humanfriendly, httpcore, googleapis-common-protos, deprecated, contourpy, chroma-hnswlib, build, anyio, watchfiles, torch, starlette, scikit-learn, rich, requests-toolbelt, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, matplotlib, llama-cpp-python, huggingface_hub, httpx, google-auth, coloredlogs, typer, tokenizers, opentelemetry-semantic-conventions, onnxruntime, langsmith, kubernetes, gradio-client, fastapi, transformers, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, gradio, sentence-transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, opentelemetry-instrumentation-fastapi, langchain, chromadb\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.6.1\n",
      "    Uninstalling importlib_metadata-8.6.1:\n",
      "      Successfully uninstalled importlib_metadata-8.6.1\n",
      "Successfully installed PyMuPDF-1.25.3 SQLAlchemy-2.0.38 aiofiles-23.2.1 annotated-types-0.7.0 anyio-4.8.0 asgiref-3.8.1 async-timeout-4.0.3 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 chroma-hnswlib-0.7.6 chromadb-0.6.3 click-8.1.8 coloredlogs-15.0.1 contourpy-1.3.0 cycler-0.12.1 deprecated-1.2.18 diskcache-5.6.3 distro-1.9.0 durationpy-0.9 fastapi-0.115.11 ffmpy-0.5.0 filelock-3.17.0 flatbuffers-25.2.10 fonttools-4.56.0 fsspec-2025.3.0 google-auth-2.38.0 googleapis-common-protos-1.69.1 gradio-4.44.1 gradio-client-1.3.0 greenlet-3.1.1 grpcio-1.70.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.1 huggingface_hub-0.29.2 humanfriendly-10.0 idna-3.10 importlib-metadata-8.5.0 importlib-resources-6.5.2 jinja2-3.1.6 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 kiwisolver-1.4.7 kubernetes-32.0.1 langchain-0.3.20 langchain-core-0.3.43 langchain-text-splitters-0.3.6 langsmith-0.3.13 llama-cpp-python-0.3.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.4 mdurl-0.1.2 mmh3-5.1.0 monotonic-1.6 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 orjson-3.10.15 overrides-7.7.0 pandas-2.2.3 pillow-10.4.0 posthog-3.19.0 protobuf-5.29.3 pyasn1-0.6.1 pyasn1-modules-0.4.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 pyparsing-3.2.1 pypdf-5.3.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 python-dotenv-1.0.1 python-multipart-0.0.20 pytz-2025.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 rich-13.9.4 rsa-4.9 ruff-0.9.10 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.13.1 semantic-version-2.10.0 sentence-transformers-3.4.1 shellingham-1.5.4 sniffio-1.3.1 starlette-0.46.1 sympy-1.13.1 tenacity-9.0.0 threadpoolctl-3.5.0 tokenizers-0.21.0 tomli-2.2.1 tomlkit-0.12.0 torch-2.6.0 tqdm-4.67.1 transformers-4.49.0 typer-0.15.2 tzdata-2025.1 urllib3-2.3.0 uvicorn-0.34.0 watchfiles-1.0.4 websocket-client-1.8.0 websockets-12.0 wrapt-1.17.2 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install llama-cpp-python gradio langchain chromadb pypdf PyMuPDF sentence-transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with GUI to upload your doc and answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 434 tensors from C:\\Users\\USER\\.cache\\huggingface\\hub\\models--dinukpathiraja--Qwen-2.5-3B-GRPO\\snapshots\\35cb297a9213a3b775e597e357bae5a277f8cfd5\\.\\DataMavericks_Qwen_2.5_3B_GRPO.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = instruct-unsloth-bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = qwen2.5\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 36\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:  181 tensors\n",
      "llama_model_loader: - type q4_K:  216 tensors\n",
      "llama_model_loader: - type q6_K:   37 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.79 GiB (4.99 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: special tokens cache size = 22\n",
      "load: token to piece cache size = 0.9310 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 36\n",
      "print_info: n_head           = 16\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 8\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 11008\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.09 B\n",
      "print_info: general.name     = Qwen2.5 3b Instruct Unsloth Bnb 4bit\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 11 ','\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151654 '<|vision_pad|>'\n",
      "print_info: LF token         = 148848 'ÄĬ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: layer  33 assigned to device CPU\n",
      "load_tensors: layer  34 assigned to device CPU\n",
      "load_tensors: layer  35 assigned to device CPU\n",
      "load_tensors: layer  36 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 434 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1834.82 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 4096\n",
      "llama_init_from_model: n_ctx_per_seq = 4096\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 256, n_embd_v_gqa = 256\n",
      "llama_kv_cache_init:        CPU KV buffer size =   144.00 MiB\n",
      "llama_init_from_model: KV self size  =  144.00 MiB, K (f16):   72.00 MiB, V (f16):   72.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   300.75 MiB\n",
      "llama_init_from_model: graph nodes  = 1266\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen2.5 3b Instruct Unsloth Bnb 4bit', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.organization': 'Unsloth', 'general.basename': 'qwen2.5', 'general.finetune': 'instruct-unsloth-bnb-4bit', 'qwen2.block_count': '36', 'general.size_label': '3B', 'qwen2.context_length': '32768', 'qwen2.embedding_length': '2048', 'general.quantization_version': '2', 'qwen2.feed_forward_length': '11008', 'qwen2.attention.head_count': '16', 'qwen2.attention.head_count_kv': '2', 'tokenizer.ggml.padding_token_id': '151654', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'general.file_type': '15', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: ,\n",
      "2025-03-10 03:47:44,953 - INFO - HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "2025-03-10 03:47:45,064 - INFO - HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-03-10 03:47:45,103 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:47:45,836 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-03-10 03:48:24,244 - INFO - Processed and indexed 30 chunks from the uploaded PDF file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Processed Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:48:28,521 - INFO - Received question: What is Group Relative Policy Optimization \n",
      "2025-03-10 03:48:28,685 - INFO - Retrieved top-3 relevant chunks: ['R1-Zero can be further augmented through the application of majority voting. For example,\\nwhen majority voting is employed on the AIME benchmark, DeepSeek-R1-Zero’s performance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nability of DeepSeek-R1-Zero to achieve such competitive performance, both with and without\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancements in reasoning tasks.\\nSelf-evolution Process of DeepSeek-R1-Zero\\nThe self-evolution process of DeepSeek-R1-Zero\\nis a fascinating demonstration of how RL can drive a model to improve its reasoning capabilities\\nautonomously. By initiating RL directly from the base model, we can closely monitor the model’s\\nprogression without the influence of the supervised fine-tuning stage. This approach provides\\na clear view of how the model evolves over time, particularly in terms of its ability to handle\\ncomplex reasoning tasks.\\nAs depicted in Figure 3, the thinking time of DeepSeek-R1-Zero shows consistent improve-\\n7\\nFigure 3 | The average response length of DeepSeek-R1-Zero on the training set during the RL\\nprocess. DeepSeek-R1-Zero naturally learns to solve reasoning tasks with more thinking time.\\nment throughout the training process. This improvement is not the result of external adjustments\\nbut rather an intrinsic development within the model. DeepSeek-R1-Zero naturally acquires the\\nability to solve increasingly complex reasoning tasks by leveraging extended test-time compu-\\ntation. This computation ranges from generating hundreds to thousands of reasoning tokens,\\nallowing the model to explore and refine its thought processes in greater depth.\\nOne of the most remarkable aspects of this self-evolution is the emergence of sophisticated\\nbehaviors as the test-time computation increases. Behaviors such as reflection—where the model\\nrevisits and reevaluates its previous steps—and the exploration of alternative approaches to', 'were the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThought reasoning process. This approach has achieved significant improvements in various\\nreasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\\nof effective test-time scaling remains an open question for the research community. Several prior\\nworks have explored various approaches, including process-based reward models (Lightman\\net al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\\nand search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\\net al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\\nperformance comparable to OpenAI’s o1 series models.\\nIn this paper, we take the first step toward improving language model reasoning capabilities\\nusing pure reinforcement learning (RL). Our goal is to explore the potential of LLMs to develop\\nreasoning capabilities without any supervised data, focusing on their self-evolution through\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuring training, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting\\nreasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\\non reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\\n71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\\nof OpenAI-o1-0912.\\nHowever, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training', '4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM)\\nPRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\\nMonte Carlo Tree Search (MCTS)\\nInspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value']\n",
      "llama_perf_context_print:        load time =   33513.59 ms\n",
      "llama_perf_context_print: prompt eval time =   33512.15 ms /  1351 tokens (   24.81 ms per token,    40.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21785.48 ms /   133 runs   (  163.80 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   55662.44 ms /  1484 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "Group Relative Policy Optimization (GRPO) is a reinforcement learning framework used in the context of this paper to improve the performance of the DeepSeek-R1-Zero model. It is not a separate concept, but rather a part of the larger RL process described in the document. GRPO is used to optimize the policy of the model, which in turn influences the model's decisions and actions during the RL process.\n",
      "</reasoning>\n",
      "<answer>\n",
      "Group Relative Policy Optimization (GRPO) is a reinforcement learning framework used in the context of this paper to improve the performance of the DeepSeek-R1-Zero model. </answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 03:50:29,938 - INFO - Received question: Can you give me the formula for GRPO using the document\n",
      "2025-03-10 03:50:30,130 - INFO - Retrieved top-3 relevant chunks: ['(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlong Chain-of-Thought (CoT) examples. 3) Distill the reasoning capability from DeepSeek-R1 to\\nsmall dense models.\\n2.2. DeepSeek-R1-Zero: Reinforcement Learning on the Base Model\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavily depended on supervised data, which are time-intensive to gather. In this section, we\\nexplore the potential of LLMs to develop reasoning capabilities without any supervised data,\\nfocusing on their self-evolution through a pure reinforcement learning process. We start with a\\nbrief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy Optimization\\nIn order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old\\npolicy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\\n𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\\n1\\n𝐺\\n𝐺\\n∑︁\\n𝑖=1\\n\\x12\\nmin\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀\\n\\x13\\n𝐴𝑖\\n\\x13\\n−𝛽D𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01\\x13\\n,\\n(1)\\nD𝐾𝐿\\n\\x00𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01 =\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −log\\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −1,\\n(2)\\nwhere 𝜀and 𝛽are hyper-parameters, and 𝐴𝑖is the advantage, computed using a group of\\nrewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:\\n𝐴𝑖= 𝑟𝑖−m𝑒𝑎𝑛({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\ns𝑡𝑑({𝑟1, 𝑟2, · · · , 𝑟𝐺})\\n.\\n(3)\\n5\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it.', 'The assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <think> </think> and\\n<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\\n<answer> answer here </answer>. User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\\nquestion during training.\\n2.2.2. Reward Modeling\\nThe reward is the source of the training signal, which decides the optimization direction of RL.\\nTo train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\\ntypes of rewards:\\n• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\\nFor example, in the case of math problems with deterministic results, the model is required\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\\nused to generate feedback based on predefined test cases.\\n• Format rewards: In addition to the accuracy reward model, we employ a format reward\\nmodel that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’\\ntags.\\nWe do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero,\\nbecause we find that the neural reward model may suffer from reward hacking in the large-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresources and it complicates the whole training pipeline.\\n2.2.3. Training Template\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequires DeepSeek-R1-Zero to first produce a reasoning process, followed by the final answer.', 'We started with the equation:\\n√︁\\n𝑎−√\\n𝑎+ 𝑥= 𝑥\\nFirst, let’s square both sides:\\n𝑎−√\\n𝑎+ 𝑥= 𝑥2 =⇒√\\n𝑎+ 𝑥= 𝑎−𝑥2\\nNext, I could square both sides again, treating the equation: · · ·\\n. . .\\nTable 3 | An interesting “aha moment” of an intermediate version of DeepSeek-R1-Zero. The\\nmodel learns to rethink using an anthropomorphic tone. This is also an aha moment for us,\\nallowing us to witness the power and beauty of reinforcement learning.\\nDrawback of DeepSeek-R1-Zero\\nAlthough DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilities and autonomously develops unexpected and powerful reasoning behaviors, it faces\\nseveral issues. For instance, DeepSeek-R1-Zero struggles with challenges like poor readability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-start data.\\n2.3. DeepSeek-R1: Reinforcement Learning with Cold Start\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoning performance be further improved or convergence accelerated by incorporating a small\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnot only produces clear and coherent Chains of Thought (CoT) but also demonstrates strong\\ngeneral capabilities? To address these questions, we design a pipeline to train DeepSeek-R1. The\\npipeline consists of four stages, outlined as follows.\\n2.3.1. Cold Start\\nUnlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodels to generate detailed answers with reflection and verification, gathering DeepSeek-R1-']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1585 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   33513.59 ms\n",
      "llama_perf_context_print: prompt eval time =   40226.46 ms /  1585 tokens (   25.38 ms per token,    39.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =  160739.30 ms /  1013 runs   (  158.68 ms per token,     6.30 tokens per second)\n",
      "llama_perf_context_print:       total time =  204176.76 ms /  2598 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The provided text doesn't give the exact formula for GRPO, but it does give the general structure and formula. The formula for the Group Relative Policy Optimization (GRPO) objective function is:\n",
      "\n",
      "\\[\n",
      "J_{GRPPO}(\\theta) = E_{q \\sim P(Q), \\{o_i\\}_G \\sim \\pi_{\\theta_0}(O|q)} \\left[\n",
      "\\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\pi_{\\theta}(o_i|q), \\frac{\\pi_{\\theta_0}(o_i|q) \\cdot \\epsilon}{1 - \\epsilon}, \\frac{\\pi_{\\theta_0}(o_i|q) \\cdot (1 + \\epsilon)}{1 + \\epsilon} \\right) - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{\\ref}) \\right) \\cdot A_i\n",
      "\\right]\n",
      "\\]\n",
      "\n",
      "Where:\n",
      "- \\( q \\sim P(Q) \\) represents sampling a question \\( q \\) from the distribution \\( P(Q) \\).\n",
      "- \\( \\{o_i\\}_G \\sim \\pi_{\\theta_0}(O|q) \\) represents sampling a group of outputs \\( \\{o_1, o_2, ..., o_G\\} \\) from the old policy \\( \\pi_{\\theta_0} \\).\n",
      "- \\( \\pi_{\\theta} \\) is the policy model.\n",
      "- \\( \\pi_{\\theta_0} \\) is the old policy.\n",
      "- \\( \\epsilon \\) and \\( \\beta \\) are hyper-parameters.\n",
      "- \\( A_i \\) is the advantage, calculated using a group of rewards \\( \\{r_1, r_2, ..., r_G\\} \\).\n",
      "- \\( D_{KL}(\\pi_\\theta || \\pi_{\\ref}) \\) is the Kullback-Leibler Divergence, which is a measure of how the distributions \\( \\pi_\\theta \\) and \\( \\pi_{\\ref} \\) differ.\n",
      "- \\( \\theta \\) is the parameters of the policy model.\n",
      "\n",
      "This formula is used to optimize the policy model \\( \\pi_{\\theta} \\) by maximizing the expected value of the objective function.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The provided text gives the structure of the Group Relative Policy Optimization (GRPO) objective function but does not directly provide the formula in a simplified form. The formula for the objective function is:\n",
      "\n",
      "\\[\n",
      "J_{GRPPO}(\\theta) = E_{q \\sim P(Q), \\{o_i\\}_G \\sim \\pi_{\\theta_0}(O|q)} \\left[\n",
      "\\frac{1}{G} \\sum_{i=1}^G \\left( \\min \\left( \\pi_{\\theta}(o_i|q), \\frac{\\pi_{\\theta_0}(o_i|q) \\cdot \\epsilon}{1 - \\epsilon}, \\frac{\\pi_{\\theta_0}(o_i|q) \\cdot (1 + \\epsilon)}{1 + \\epsilon} \\right) - \\beta \\cdot D_{KL}(\\pi_\\theta || \\pi_{\\ref}) \\right) \\cdot A_i\n",
      "\\right]\n",
      "\\]\n",
      "\n",
      "Where:\n",
      "- \\( q \\sim P(Q) \\) represents sampling a question \\( q \\) from the distribution \\( P(Q) \\).\n",
      "- \\( \\{o_i\\}_G \\sim \\pi_{\\theta_0}(O|q) \\) represents sampling a group of outputs \\( \\{o_1, o_2, ..., o_G\\} \\) from the old policy \\( \\pi_{\\theta_0} \\).\n",
      "- \\( \\pi_{\\theta} \\) is the policy model.\n",
      "- \\( \\pi_{\\theta_0} \\) is the old policy.\n",
      "- \\( \\epsilon \\) and \\( \\beta \\) are hyper-parameters.\n",
      "- \\( A_i \\) is the advantage, calculated using a group of rewards \\( \\{r_1, r_2, ..., r_G\\} \\).\n",
      "- \\( D_{KL}(\\pi_\\theta || \\pi_{\\ref}) \\) is the Kullback-Leibler Divergence, which is a measure of how the distributions \\( \\pi_\\theta \\) and \\( \\pi_{\\ref} \\) differ.\n",
      "- \\( \\theta \\) is the parameters of the policy model.\n",
      "\n",
      "The \\( \\pi_{\\theta}(o_i|q) \\) within the \\(\\min\\) function represents the policy model's action \\( o_i \\) given question \\( q \\). The objective function is maximized to optimize the policy model \\( \\pi_{\\theta} \\).\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "# RAG Application\n",
    "\n",
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load LLM model (Adjust path accordingly)\n",
    "llm = Llama.from_pretrained(\n",
    "        repo_id=\"dinukpathiraja/Qwen-2.5-3B-GRPO\",\n",
    "        filename=\"DataMavericks_Qwen_2.5_3B_GRPO.gguf\",\n",
    "        n_ctx=4096,  \n",
    "        n_threads=4,   \n",
    "        n_gpu_layers=-1\n",
    "    )\n",
    "\n",
    "# Load ColBERT model and tokenizer\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Device configuration for ColBERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "colbert_model = colbert_model.to(device)\n",
    "\n",
    "# Initialize storage for embeddings and text chunks\n",
    "file_embeddings = []\n",
    "file_text_chunks = []\n",
    "\n",
    "## File Processing Function (Supports .pdf, .txt, .md)\n",
    "def process_file(uploaded_file):\n",
    "    try:\n",
    "        file_extension = uploaded_file.name.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(uploaded_file.name)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(uploaded_file.name, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"❌ Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        print(\"PDF Processed Successfully\")\n",
    "        return f\"✅ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"❌ Error processing file: {str(e)}\"\n",
    "\n",
    "\n",
    "## Retrieve Context Using ColBERT\n",
    "def get_context(question):\n",
    "    try:\n",
    "        # Encode the query using ColBERT\n",
    "        inputs = colbert_tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
    "\n",
    "        # Compute similarity scores between query and stored embeddings\n",
    "        scores = [torch.cosine_similarity(query_embedding, emb, dim=1).item() for emb in file_embeddings]\n",
    "\n",
    "        # Get top-3 most relevant chunks based on similarity scores\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
    "        top_chunks = [file_text_chunks[i] for i in top_indices]\n",
    "\n",
    "        logging.info(f\"Retrieved top-3 relevant chunks: {top_chunks}\")\n",
    "        \n",
    "        return \"\\n\".join(top_chunks) if top_chunks else \"⚠️ No relevant context found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving context: {e}\")\n",
    "        return \"⚠️ Error retrieving context.\"\n",
    "\n",
    "\n",
    "def answer_question(question, chat_history):\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return chat_history\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            # Check if <answer> tags exist properly\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                # ELSE condition: <answer> tags not found correctly\n",
    "                # Take everything after </reasoning>, remove any leftover tags\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                # Clean up any accidental tags\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "\n",
    "        else:\n",
    "            # If reasoning tags are missing entirely, treat whole text as answer\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "        # Combine reasoning and answer for display\n",
    "        formatted_response = f\"**Reasoning:**\\n{reasoning}\\n\\n**Answer:**\\n{answer}\"\n",
    "\n",
    "        # Append to chat history\n",
    "        chat_history.append((question, formatted_response))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "\n",
    "# Gradio UI (Continuous Chat Session with loading indicators)\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\"# 📄 Enhanced RAG Chatbot with Multi-format Support\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document (.pdf/.txt/.md) 📁\")\n",
    "    \n",
    "    upload_status = gr.Label(\"\")\n",
    "    upload_btn = gr.Button(\"Upload & Process File 🚀\")\n",
    "\n",
    "    upload_btn.click(\n",
    "        fn=process_file,\n",
    "        inputs=[file_input],\n",
    "        outputs=[upload_status]\n",
    "    )\n",
    "\n",
    "    chatbot_ui = gr.Chatbot(label=\"Chat History 💬\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Enter your question ❓\")\n",
    "    \n",
    "    ask_btn = gr.Button(\"Get Answer ✨\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot_ui],\n",
    "        outputs=[chatbot_ui]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.26.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.31)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (2.10.6)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.60.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (2.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (2.0.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (2.7.1)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n",
      "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
      "Downloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
      "Installing collected packages: langchain-core, langchain_openai, ragas\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.31\n",
      "    Uninstalling langchain-core-0.3.31:\n",
      "      Successfully uninstalled langchain-core-0.3.31\n",
      "Successfully installed langchain-core-0.3.43 langchain_openai-0.3.8 ragas-0.2.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.5.7 requires fastapi==0.111.0, but you have fastapi 0.115.8 which is incompatible.\n",
      "open-webui 0.5.7 requires pydantic==2.9.2, but you have pydantic 2.10.6 which is incompatible.\n",
      "open-webui 0.5.7 requires unstructured==0.15.9, but you have unstructured 0.16.23 which is incompatible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:25:08,033 - INFO - Received question: Can you summerize the  DeepSeek-R1 Evaluation Percentages and Number Values\n",
      "2025-03-09 19:25:08,103 - INFO - Retrieved top-3 relevant chunks: ['We intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases—such as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies—to ensure that we can accurately observe the model’s natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero\\nFigure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model’s performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI’s o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nOpenAI-o1-0912\\n74.4\\n83.3\\n94.8\\n77.3\\n63.4\\n1843\\nDeepSeek-R1-Zero\\n71.0\\n86.7\\n95.9\\n73.3\\n50.0\\n1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model’s ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-', 'C-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel’s ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1’s strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', 'as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   74468.74 ms /  1503 tokens (   49.55 ms per token,    20.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =  135033.31 ms /   599 runs   (  225.43 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =  212533.28 ms /  2102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The question asks to summarize the evaluation percentages and number values for DeepSeek-R1. To answer it, I will look at the tables and figures provided in the text, and extract the relevant information.\n",
      "</reasoning>\n",
      "<answer>\n",
      "From the text, we can see the following key information:\n",
      "\n",
      "1. **AIME 2024 Benchmark:**\n",
      "   - Initial Average pass@1 score: 15.6%\n",
      "   - Final Average pass@1 score: 71.0%\n",
      "\n",
      "2. **Other Benchmarks:**\n",
      "   - **Math-500 Benchmark:**\n",
      "     - OpenAI-o1-0912: 83.3%\n",
      "     - DeepSeek-R1-Zero: 86.7%\n",
      "\n",
      "   - **GPQA Benchmark:**\n",
      "     - OpenAI-o1-0912: 94.8%\n",
      "     - DeepSeek-R1-Zero: 95.9%\n",
      "\n",
      "   - **LiveCode Benchmark:**\n",
      "     - OpenAI-o1-0912: 77.3%\n",
      "     - DeepSeek-R1-Zero: 73.3%\n",
      "\n",
      "   - **Codeforces Benchmark:**\n",
      "     - OpenAI-o1-0912: 63.8%\n",
      "     - DeepSeek-R1-Zero: 1444 (Elo rating)\n",
      "\n",
      "   - **Diamond Benchmark:**\n",
      "     - OpenAI-o1-0912: 50.0%\n",
      "     - DeepSeek-R1-Zero: 50.0%\n",
      "\n",
      "   - **FRAMES Benchmark:**\n",
      "     - OpenAI-o1-0912: 1843\n",
      "     - DeepSeek-R1-Zero: 1444\n",
      "\n",
      "   - **SimpleQA Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **IF-Eval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **AlpacaEval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 92.3%\n",
      "\n",
      "   - **ArenaHard Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 87.6%\n",
      "\n",
      "The DeepSeek-R1-Zero model has significantly outperformed OpenAI-o1-0912 on multiple benchmarks, especially on Math-500, GPQA, and LiveCode, with an impressive Elo rating of 1444 on Codeforces. On the AIME 2024 benchmark, it improved from 15.6% to 71.0%. </answer>\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 20:09:11,321 - INFO - Processed and indexed 5 chunks from the uploaded MD file.\n",
      "2025-03-09 20:09:11,323 - INFO - Received question: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "2025-03-09 20:09:11,347 - INFO - Retrieved top-3 relevant chunks: ['# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\\n| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\\n| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\\n\\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1096 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers using the RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =  388706.45 ms /  1609 tokens (  241.58 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55136.70 ms /   264 runs   (  208.85 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  101516.50 ms /  1873 tokens\n",
      "2025-03-09 20:10:52,919 - INFO - Received question: What are the two expert load-balancing strategies in EPLB?\n",
      "2025-03-09 20:10:52,994 - INFO - Retrieved top-3 relevant chunks: [\"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\", '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1062 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "<answer>\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   42840.11 ms /  1062 tokens (   40.34 ms per token,    24.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59078.96 ms /   272 runs   (  217.20 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  103124.48 ms /  1334 tokens\n",
      "2025-03-09 20:12:36,152 - INFO - Received question: What architectural features make 3FS suitable for AI workloads?\n",
      "2025-03-09 20:12:36,224 - INFO - Retrieved top-3 relevant chunks: ['### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1130 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The context provided describes the EPLB (Expert Parallelism Load Balancer) and outlines two different load-balancing strategies for handling varying expert loads across GPUs. These strategies are based on the number of server nodes and the number of expert groups, and they are used to distribute the experts (in terms of replication and placement) to ensure load balancing across GPUs. The context mentions that there are two policies: one for hierarchical load balancing and one for global load balancing. This information directly answers the question about the two expert load-balancing strategies in EPLB.</reasoning>\n",
      "<answer>\n",
      "In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "</answer>  \n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   49799.38 ms /  1130 tokens (   44.07 ms per token,    22.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  123701.56 ms /   778 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  176817.92 ms /  1908 tokens\n",
      "2025-03-09 20:15:33,063 - INFO - Received question: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "2025-03-09 20:15:33,123 - INFO - Retrieved top-3 relevant chunks: ['## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(𝐹+𝐵)            | 1×        | PP         |\\n| ZB1P      | (PP-1)(𝐹+𝐵-2𝑊)         | 1×        | PP         |\\n| DualPipe  | (PP/2-1)(𝐹&𝐵+𝐵-3𝑊)     | 2×        | PP+1       |\\n\\n𝐹 denotes the execution time of a forward chunk, 𝐵 denotes the execution time of a full backward chunk, 𝑊 denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1090 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The Fire-Flyer File System (3FS) is designed to be highly suitable for AI workloads, particularly given the high-throughput and strong consistency requirements. Several architectural features of 3FS make it a good fit for AI workloads, including its:\n",
      "\n",
      "1. Disaggregated Architecture: The system combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resources in a locality-oblivious manner. This feature is particularly useful in AI workloads where data access patterns are highly irregular and can vary significantly across different nodes.\n",
      "\n",
      "2. Strong Consistency: The system implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency. This feature simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. File Interfaces: Developing stateless metadata services backed by a transactional key-value store (e.g., FoundationDB) makes the file interface well-known and used everywhere, reducing the learning curve for new users.\n",
      "\n",
      "4. Data Preparation: Organizing outputs of data analytics pipelines into hierarchical directory structures and managing large volumes of intermediate outputs efficiently, which is beneficial for AI workloads dealing with large datasets and complex data pipelines.\n",
      "\n",
      "5. Dataloaders: Eliminating the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes. This feature simplifies the process of accessing large datasets, which is common in AI workloads like training and inference.\n",
      "\n",
      "6. Checkpointing: Supporting high-throughput parallel checkpointing for large-scale training, which is essential for AI training workflows that often involve iterative training and checkpointing.\n",
      "\n",
      "7. KVCache for Inference: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity, which is crucial for optimizing the inference process in AI systems.\n",
      "\n",
      "These features collectively make 3FS a suitable architectural choice for AI workloads, especially in environments that require high-throughput, strong consistency, and efficient data management.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   34992.54 ms /  1090 tokens (   32.10 ms per token,    31.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26253.23 ms /   175 runs   (  150.02 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   61777.28 ms /  1265 tokens\n",
      "2025-03-09 20:16:34,915 - INFO - Received question: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "2025-03-09 20:16:34,992 - INFO - Retrieved top-3 relevant chunks: ['## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2×200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2×400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1×200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ’s actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches — meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1128 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The given context does not provide any information about 3FS (which seems to be a typo or misunderstanding, as the context is about DeepSeek-V3 and DualPipe), nor does it mention KVCache optimization for LLM inference. There is no data or information related to 3FS or KVCache optimization in the provided context. Therefore, based on the information given, it's impossible to answer the question about 3FS and KVCache optimization.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   32910.38 ms /  1128 tokens (   29.18 ms per token,    34.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44383.15 ms /   300 runs   (  147.94 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   78335.24 ms /  1428 tokens\n",
      "2025-03-09 20:17:53,343 - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The GraySort benchmark results demonstrate 3FS's capabilities by showcasing how efficiently it can handle large-scale data sorting tasks. In this case, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,19gestions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The ability to sort such a large dataset in a relatively short time demonstrates 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</reasoning>\n",
      "<answer>\n",
      "The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</answer>\n",
      "Evaluating generated answers...\n",
      "\n",
      "Evaluation Results:\n",
      "Question 1: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "Generated Answer: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "Reference Answer: DualPipe reduces pipeline bubbles to $$(PP/2-1)(F\\&B + B - 3W)$$ using bidirectional parallelism, while 1F1B and ZB1P have bubbles of $$(PP-1)(F+B)$$ and $$(PP-1)(F+B-2W)$$ respectively. It uses 2× parameter memory and PP+1 activation memory, enabling full computation-communication overlap for forward/backward phases.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.16568047337278108, recall=0.5384615384615384, fmeasure=0.25339366515837103), 'rouge2': Score(precision=0.023809523809523808, recall=0.0784313725490196, fmeasure=0.0365296803652968), 'rougeL': Score(precision=0.11242603550295859, recall=0.36538461538461536, fmeasure=0.17194570135746606)}\n",
      "Question 2: What are the two expert load-balancing strategies in EPLB?\n",
      "Generated Answer: In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "Reference Answer: EPLB employs: 1) **Hierarchical Load Balancing** (groups experts by nodes to minimize inter-node traffic, used when nodes divide expert groups evenly), and 2) **Global Load Balancing** (replicates experts across all GPUs regardless of groups, typically for decoding stages with larger EP sizes).\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.2916666666666667, recall=0.813953488372093, fmeasure=0.4294478527607362), 'rouge2': Score(precision=0.11764705882352941, recall=0.3333333333333333, fmeasure=0.1739130434782609), 'rougeL': Score(precision=0.225, recall=0.627906976744186, fmeasure=0.33128834355828224)}\n",
      "Question 3: What architectural features make 3FS suitable for AI workloads?\n",
      "Generated Answer: The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "Reference Answer: 3FS combines disaggregated RDMA/SSD storage with CRAQ-based strong consistency. It provides standard file interfaces via FoundationDB-managed metadata, supports dataloaders/checkpointing/KVCache, and delivers 6.6 TiB/s aggregate read throughput in production clusters.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.08270676691729323, recall=0.6111111111111112, fmeasure=0.14569536423841062), 'rouge2': Score(precision=0.007547169811320755, recall=0.05714285714285714, fmeasure=0.013333333333333332), 'rougeL': Score(precision=0.06015037593984962, recall=0.4444444444444444, fmeasure=0.10596026490066225)}\n",
      "Question 4: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "Generated Answer: The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "Reference Answer: 3FS achieves 40 GiB/s peak KVCache read throughput with efficient garbage collection (high IOPS removal operations). It serves as a cost-effective DRAM alternative, handling 6.6 TiB/s read throughput across 500+ client nodes in production.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.1111111111111111, recall=0.15789473684210525, fmeasure=0.13043478260869565), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.09259259259259259, recall=0.13157894736842105, fmeasure=0.10869565217391304)}\n",
      "Question 5: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "Generated Answer: The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.\n",
      "Reference Answer: 3FS sorted 110.5 TiB of data in 30m14s using a two-phase shuffle-and-sort approach, achieving 3.66 TiB/min throughput across 25 storage/50 compute nodes. This demonstrates its large-scale data processing efficiency for AI workloads.\n",
      "BLEU Score: 0.0393\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.24299065420560748, recall=0.65, fmeasure=0.35374149659863946), 'rouge2': Score(precision=0.10377358490566038, recall=0.28205128205128205, fmeasure=0.15172413793103448), 'rougeL': Score(precision=0.12149532710280374, recall=0.325, fmeasure=0.17687074829931973)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import logging\n",
    "import os \n",
    "\n",
    "# Define the process_file function\n",
    "def process_file_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Processes a file based on its extension (.pdf, .txt, .md) and indexes its content into a vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        str: Success or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"❌ File not found: {file_path}\"\n",
    "\n",
    "        # Determine the file extension\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(file_path)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"❌ Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        return f\"✅ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"❌ Error processing file: {str(e)}\"\n",
    "\n",
    "# Load JSON Dataset\n",
    "def load_dataset(json_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a JSON file.\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: Loaded dataset as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated answer from the RAG system.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: List of generated answers.\n",
    "    \"\"\"\n",
    "    generated_answers = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        generated_answer = rag_system(question, chat_history)\n",
    "        generated_answers.append(generated_answer)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "# Evaluate Generated Answers\n",
    "def evaluate_answers(generated_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using BLEU and ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        generated_answers (list): List of answers generated by the RAG system.\n",
    "        reference_answers (list): List of reference answers from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing BLEU and ROUGE scores for each answer pair.\n",
    "    \"\"\"\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results = {\"bleu_scores\": [], \"rouge_scores\": []}\n",
    "\n",
    "    for gen_answer, ref_answer in zip(generated_answers, reference_answers):\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([ref_answer.split()], gen_answer.split())\n",
    "        results[\"bleu_scores\"].append(bleu_score)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(ref_answer, gen_answer)\n",
    "        results[\"rouge_scores\"].append(rouge_scores)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "    if file:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(json_path)\n",
    "\n",
    "        # Extract questions and reference answers\n",
    "        questions = [entry[\"question\"] for entry in dataset]\n",
    "        reference_answers = [entry[\"reference_answer\"] for entry in dataset]\n",
    "\n",
    "        # Generate answers using RAG system\n",
    "        print(\"Generating answers using the RAG system...\")\n",
    "        generated_answers = generate_answers(rag_system, dataset)\n",
    "\n",
    "        # Evaluate generated answers against reference answers\n",
    "        print(\"Evaluating generated answers...\")\n",
    "        evaluation_results = evaluate_answers(generated_answers, reference_answers)\n",
    "\n",
    "        # Print Results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        \n",
    "        for i, (question, gen_answer, ref_answer) in enumerate(zip(questions, generated_answers, reference_answers)):\n",
    "            print(f\"Question {i+1}: {question}\")\n",
    "            print(f\"Generated Answer: {gen_answer}\")\n",
    "            print(f\"Reference Answer: {ref_answer}\")\n",
    "            print(f\"BLEU Score: {evaluation_results['bleu_scores'][i]:.4f}\")\n",
    "            print(f\"ROUGE Scores: {evaluation_results['rouge_scores'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.language_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     Faithfulness,\n\u001b[0;32m      4\u001b[0m     AnswerRelevance,\n\u001b[0;32m      5\u001b[0m     ContextRelevance,\n\u001b[0;32m      6\u001b[0m     AnswerSimilarity,\n\u001b[0;32m      7\u001b[0m     FactualCorrectness\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_metrics\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamples\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTurnSample\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheInterface, DiskCacheBackend, cacher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\evaluation.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCallbackHandler, BaseCallbackManager\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings \u001b[38;5;28;01mas\u001b[39;00m LangchainEmbeddings\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel \u001b[38;5;28;01mas\u001b[39;00m LangchainLLM\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_was_completed\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.language_models'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevance,\n",
    "    ContextRelevance,\n",
    "    AnswerSimilarity,\n",
    "    FactualCorrectness\n",
    ")\n",
    "from ragas.evaluation import evaluate_metrics\n",
    "from ragas.samples import SingleTurnSample\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Generated answer and retrieved context.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"⚠️ No relevant context found.\":\n",
    "        error_msg = \"⚠️ No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract reasoning and answer from response\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    return {\"answer\": answer, \"context\": context}\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing generated answers and contexts.\n",
    "    \"\"\"\n",
    "    generated_data = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        result = rag_system(question, chat_history)\n",
    "        \n",
    "        generated_data.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": result[\"answer\"],\n",
    "            \"retrieved_context\": result[\"context\"],\n",
    "            \"reference_answer\": entry.get(\"reference_answer\", \"\")\n",
    "        })\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "# Evaluate Answers Using RAGAS Metrics\n",
    "def evaluate_ragas(generated_data):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using RAGAS metrics.\n",
    "    \n",
    "    Args:\n",
    "        generated_data (list): List of dictionaries containing questions, generated answers, contexts, and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing scores for all RAGAS metrics.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for data in generated_data:\n",
    "        sample = SingleTurnSample(\n",
    "            question=data[\"question\"],\n",
    "            retrieved_context=data[\"retrieved_context\"],\n",
    "            ground_truth=data[\"reference_answer\"],\n",
    "            prediction=data[\"generated_answer\"]\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Define metrics to evaluate\n",
    "    metrics = {\n",
    "        \"Faithfulness\": Faithfulness(),\n",
    "        \"Answer Relevance\": AnswerRelevance(),\n",
    "        \"Context Relevance\": ContextRelevance(),\n",
    "        \"Answer Similarity\": AnswerSimilarity(),\n",
    "        \"Factual Correctness\": FactualCorrectness()\n",
    "    }\n",
    "\n",
    "    # Evaluate metrics for all samples\n",
    "    results = evaluate_metrics(samples, metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "\n",
    "    # Load dataset\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Generate answers using RAG system\n",
    "    print(\"Generating answers using the RAG system...\")\n",
    "    generated_data = generate_answers(rag_system, dataset)\n",
    "\n",
    "    # Evaluate generated answers using RAGAS metrics\n",
    "    print(\"Evaluating generated answers...\")\n",
    "    evaluation_results = evaluate_ragas(generated_data)\n",
    "\n",
    "    # Print Results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    \n",
    "    for metric_name, score in evaluation_results.items():\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
