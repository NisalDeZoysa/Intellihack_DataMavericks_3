{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python gradio langchain chromadb pypdf PyMuPDF sentence-transformers huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with GUI to upload your doc and answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cuda available\n",
    "torch.cuda.is_available()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No file found in dinukpathiraja/Qwen-2.5-3B-GRPO that match DataMavericks_Qwen_2.5_3B_GRPO.gguf\n\nAvailable Files:\n[\".gitattributes\", \"Qwen_3B_GRPO_750.gguf\", \"Qwen_3B_GRPO_Enabled.gguf\", \"README.md\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%(asctime)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(levelname)s\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%(message)s\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load LLM model (Adjust path accordingly)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdinukpathiraja/Qwen-2.5-3B-GRPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataMavericks_Qwen_2.5_3B_GRPO.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load ColBERT model and tokenizer\u001b[39;00m\n\u001b[0;32m     22\u001b[0m colbert_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolbert-ir/colbertv2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\llama_cpp\\llama.py:2288\u001b[0m, in \u001b[0;36mLlama.from_pretrained\u001b[1;34m(cls, repo_id, filename, additional_files, local_dir, local_dir_use_symlinks, cache_dir, **kwargs)\u001b[0m\n\u001b[0;32m   2285\u001b[0m matching_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_list \u001b[38;5;28;01mif\u001b[39;00m fnmatch\u001b[38;5;241m.\u001b[39mfnmatch(file, filename)]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching_files) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that match \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable Files:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(file_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2291\u001b[0m     )\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matching_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m matching \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable Files:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;241m.\u001b[39mdumps(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2297\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No file found in dinukpathiraja/Qwen-2.5-3B-GRPO that match DataMavericks_Qwen_2.5_3B_GRPO.gguf\n\nAvailable Files:\n[\".gitattributes\", \"Qwen_3B_GRPO_750.gguf\", \"Qwen_3B_GRPO_Enabled.gguf\", \"README.md\"]"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llama_cpp import Llama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import logging\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load LLM model (Adjust path accordingly)\n",
    "llm = Llama.from_pretrained(\n",
    "        repo_id=\"dinukpathiraja/Qwen-2.5-3B-GRPO\",\n",
    "        filename=\"DataMavericks_Qwen_2.5_3B_GRPO.gguf\",\n",
    "        n_ctx=4096,  \n",
    "        n_threads=4,   \n",
    "        n_gpu_layers=-1\n",
    "    )\n",
    "\n",
    "# Load ColBERT model and tokenizer\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "\n",
    "# Device configuration for ColBERT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "colbert_model = colbert_model.to(device)\n",
    "\n",
    "# Initialize storage for embeddings and text chunks\n",
    "file_embeddings = []\n",
    "file_text_chunks = []\n",
    "\n",
    "## File Processing Function (Supports .pdf, .txt, .md)\n",
    "def process_file(uploaded_file):\n",
    "    try:\n",
    "        file_extension = uploaded_file.name.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(uploaded_file.name)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(uploaded_file.name, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"‚ùå Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        print(\"PDF Processed Successfully\")\n",
    "        return f\"‚úÖ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"‚ùå Error processing file: {str(e)}\"\n",
    "\n",
    "\n",
    "## Retrieve Context Using ColBERT\n",
    "def get_context(question):\n",
    "    try:\n",
    "        # Encode the query using ColBERT\n",
    "        inputs = colbert_tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1).cpu()\n",
    "\n",
    "        # Compute similarity scores between query and stored embeddings\n",
    "        scores = [torch.cosine_similarity(query_embedding, emb, dim=1).item() for emb in file_embeddings]\n",
    "\n",
    "        # Get top-3 most relevant chunks based on similarity scores\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]\n",
    "        top_chunks = [file_text_chunks[i] for i in top_indices]\n",
    "\n",
    "        logging.info(f\"Retrieved top-3 relevant chunks: {top_chunks}\")\n",
    "        \n",
    "        return \"\\n\".join(top_chunks) if top_chunks else \"‚ö†Ô∏è No relevant context found.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving context: {e}\")\n",
    "        return \"‚ö†Ô∏è Error retrieving context.\"\n",
    "\n",
    "\n",
    "def answer_question(question, chat_history):\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)\n",
    "\n",
    "    if not context or context == \"‚ö†Ô∏è No relevant context found.\":\n",
    "        error_msg = \"‚ö†Ô∏è No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return chat_history\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            # Check if <answer> tags exist properly\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                # ELSE condition: <answer> tags not found correctly\n",
    "                # Take everything after </reasoning>, remove any leftover tags\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                # Clean up any accidental tags\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "\n",
    "        else:\n",
    "            # If reasoning tags are missing entirely, treat whole text as answer\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "        # Combine reasoning and answer for display\n",
    "        formatted_response = f\"**Reasoning:**\\n{reasoning}\\n\\n**Answer:**\\n{answer}\"\n",
    "\n",
    "        # Append to chat history\n",
    "        chat_history.append((question, formatted_response))\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "\n",
    "# Gradio UI (Continuous Chat Session with loading indicators)\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    gr.Markdown(\"# üìÑ Enhanced RAG Chatbot with Multi-format Support\")\n",
    "\n",
    "    with gr.Row():\n",
    "        file_input = gr.File(label=\"Upload Document (.pdf/.txt/.md) üìÅ\")\n",
    "    \n",
    "    upload_status = gr.Label(\"\")\n",
    "    upload_btn = gr.Button(\"Upload & Process File üöÄ\")\n",
    "\n",
    "    upload_btn.click(\n",
    "        fn=process_file,\n",
    "        inputs=[file_input],\n",
    "        outputs=[upload_status]\n",
    "    )\n",
    "\n",
    "    chatbot_ui = gr.Chatbot(label=\"Chat History üí¨\")\n",
    "    \n",
    "    question_input = gr.Textbox(label=\"Enter your question ‚ùì\")\n",
    "    \n",
    "    ask_btn = gr.Button(\"Get Answer ‚ú®\")\n",
    "\n",
    "    ask_btn.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot_ui],\n",
    "        outputs=[chatbot_ui]\n",
    "    )\n",
    "\n",
    "demo.queue().launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.26.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (3.2.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.8.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.31)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (0.3.7)\n",
      "Collecting langchain_openai (from ragas)\n",
      "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (2.10.6)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ragas) (1.60.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from openai>1->ragas) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from pydantic>=2->ragas) (2.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\llms\\lib\\site-packages (from datasets->ragas) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from datasets->ragas) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (2.0.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (0.1.147)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain->ragas) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community->ragas) (2.7.1)\n",
      "Collecting langchain-core (from ragas)\n",
      "  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n",
      "Downloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n",
      "Downloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n",
      "Installing collected packages: langchain-core, langchain_openai, ragas\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.31\n",
      "    Uninstalling langchain-core-0.3.31:\n",
      "      Successfully uninstalled langchain-core-0.3.31\n",
      "Successfully installed langchain-core-0.3.43 langchain_openai-0.3.8 ragas-0.2.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-webui 0.5.7 requires fastapi==0.111.0, but you have fastapi 0.115.8 which is incompatible.\n",
      "open-webui 0.5.7 requires pydantic==2.9.2, but you have pydantic 2.10.6 which is incompatible.\n",
      "open-webui 0.5.7 requires unstructured==0.15.9, but you have unstructured 0.16.23 which is incompatible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 19:25:08,033 - INFO - Received question: Can you summerize the  DeepSeek-R1 Evaluation Percentages and Number Values\n",
      "2025-03-09 19:25:08,103 - INFO - Retrieved top-3 relevant chunks: ['We intentionally limit our constraints to this structural format, avoiding any content-specific\\nbiases‚Äîsuch as mandating reflective reasoning or promoting particular problem-solving strate-\\ngies‚Äîto ensure that we can accurately observe the model‚Äôs natural progression during the RL\\nprocess.\\n2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero\\nPerformance of DeepSeek-R1-Zero\\nFigure 2 depicts the performance trajectory of DeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as the\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumping from an initial 15.6% to an impressive 71.0%, reaching performance levels\\ncomparable to OpenAI-o1-0912. This significant improvement highlights the efficacy of our RL\\nalgorithm in optimizing the model‚Äôs performance over time.\\nTable 2 provides a comparative analysis between DeepSeek-R1-Zero and OpenAI‚Äôs o1-0912\\nmodels across a variety of reasoning-related benchmarks. The findings reveal that RL empowers\\n6\\nModel\\nAIME 2024\\nMATH-500\\nGPQA\\nLiveCode\\nCodeForces\\nDiamond\\nBench\\npass@1\\ncons@64\\npass@1\\npass@1\\npass@1\\nrating\\nOpenAI-o1-mini\\n63.6\\n80.0\\n90.0\\n60.0\\n53.8\\n1820\\nOpenAI-o1-0912\\n74.4\\n83.3\\n94.8\\n77.3\\n63.4\\n1843\\nDeepSeek-R1-Zero\\n71.0\\n86.7\\n95.9\\n73.3\\n50.0\\n1444\\nTable 2 | Comparison of DeepSeek-R1-Zero and OpenAI o1 models on reasoning-related\\nbenchmarks.\\nFigure 2 | AIME accuracy of DeepSeek-R1-Zero during training. For each question, we sample\\n16 responses and calculate the overall average accuracy to ensure a stable evaluation.\\nDeepSeek-R1-Zero to attain robust reasoning capabilities without the need for any supervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the model‚Äôs ability to\\nlearn and generalize effectively through RL alone. Additionally, the performance of DeepSeek-', 'C-SimpleQA (Correct)\\n55.4\\n58.7\\n68.0\\n40.3\\n-\\n63.7\\nTable 4 | Comparison between DeepSeek-R1 and other representative models.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond, DeepSeek-R1 demonstrates superior performance compared to DeepSeek-V3. This im-\\nprovement is primarily attributed to enhanced accuracy in STEM-related questions, where signif-\\nicant gains are achieved through large-scale reinforcement learning. Additionally, DeepSeek-R1\\nexcels on FRAMES, a long-context-dependent QA task, showcasing its strong document analysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysis tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed where\\nOpenAI-o1 surpasses GPT-4o on this benchmark. However, DeepSeek-R1 performs worse than\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nanswering certain queries after safety RL. Without safety RL, DeepSeek-R1 could achieve an\\naccuracy of over 70%.\\nDeepSeek-R1 also delivers impressive results on IF-Eval, a benchmark designed to assess a\\nmodel‚Äôs ability to follow format instructions. These improvements can be linked to the inclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore, remarkable performance is observed on AlpacaEval2.0 and ArenaHard,\\nindicating DeepSeek-R1‚Äôs strengths in writing tasks and open-domain question answering. Its\\nsignificant outperformance of DeepSeek-V3 underscores the generalization benefits of large-scale\\nRL, which not only boosts reasoning capabilities but also improves performance across diverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverage of 689 tokens on ArenaHard and 2,218 characters on AlpacaEval 2.0. This indicates that\\n13', 'as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\\nthe competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\\nDeepSeek-V3, which could help developers in real world tasks.\\n‚Ä¢ Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\\nsurpasses other closed-source models, demonstrating its competitive edge in educational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstrating its capability in handling fact-based queries. A similar trend is observed\\nwhere OpenAI-o1 surpasses 4o on this benchmark.\\n4\\n‚Ä¢ Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneral question answering, editing, summarization, and more. It achieves an impressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-context understanding, substantially outperforming DeepSeek-V3 on long-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   74468.74 ms /  1503 tokens (   49.55 ms per token,    20.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =  135033.31 ms /   599 runs   (  225.43 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =  212533.28 ms /  2102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The question asks to summarize the evaluation percentages and number values for DeepSeek-R1. To answer it, I will look at the tables and figures provided in the text, and extract the relevant information.\n",
      "</reasoning>\n",
      "<answer>\n",
      "From the text, we can see the following key information:\n",
      "\n",
      "1. **AIME 2024 Benchmark:**\n",
      "   - Initial Average pass@1 score: 15.6%\n",
      "   - Final Average pass@1 score: 71.0%\n",
      "\n",
      "2. **Other Benchmarks:**\n",
      "   - **Math-500 Benchmark:**\n",
      "     - OpenAI-o1-0912: 83.3%\n",
      "     - DeepSeek-R1-Zero: 86.7%\n",
      "\n",
      "   - **GPQA Benchmark:**\n",
      "     - OpenAI-o1-0912: 94.8%\n",
      "     - DeepSeek-R1-Zero: 95.9%\n",
      "\n",
      "   - **LiveCode Benchmark:**\n",
      "     - OpenAI-o1-0912: 77.3%\n",
      "     - DeepSeek-R1-Zero: 73.3%\n",
      "\n",
      "   - **Codeforces Benchmark:**\n",
      "     - OpenAI-o1-0912: 63.8%\n",
      "     - DeepSeek-R1-Zero: 1444 (Elo rating)\n",
      "\n",
      "   - **Diamond Benchmark:**\n",
      "     - OpenAI-o1-0912: 50.0%\n",
      "     - DeepSeek-R1-Zero: 50.0%\n",
      "\n",
      "   - **FRAMES Benchmark:**\n",
      "     - OpenAI-o1-0912: 1843\n",
      "     - DeepSeek-R1-Zero: 1444\n",
      "\n",
      "   - **SimpleQA Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **IF-Eval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 63.7\n",
      "\n",
      "   - **AlpacaEval Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 92.3%\n",
      "\n",
      "   - **ArenaHard Benchmark:**\n",
      "     - OpenAI-o1-0912: -\n",
      "     - DeepSeek-R1-Zero: 87.6%\n",
      "\n",
      "The DeepSeek-R1-Zero model has significantly outperformed OpenAI-o1-0912 on multiple benchmarks, especially on Math-500, GPQA, and LiveCode, with an impressive Elo rating of 1444 on Codeforces. On the AIME 2024 benchmark, it improved from 15.6% to 71.0%. </answer>\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 20:09:11,321 - INFO - Processed and indexed 5 chunks from the uploaded MD file.\n",
      "2025-03-09 20:09:11,323 - INFO - Received question: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "2025-03-09 20:09:11,347 - INFO - Retrieved top-3 relevant chunks: ['# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(ùêπ+ùêµ)            | 1√ó        | PP         |\\n| ZB1P      | (PP-1)(ùêπ+ùêµ-2ùëä)         | 1√ó        | PP         |\\n| DualPipe  | (PP/2-1)(ùêπ&ùêµ+ùêµ-3ùëä)     | 2√ó        | PP+1       |\\n\\nùêπ denotes the execution time of a forward chunk, ùêµ denotes the execution time of a full backward chunk, ùëä denotes the execution time of a \"backward for weights\" chunk, and ùêπ&ùêµ denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ‚Äôs actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches ‚Äî meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1096 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answers using the RAG system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =  388706.45 ms /  1609 tokens (  241.58 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55136.70 ms /   264 runs   (  208.85 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  101516.50 ms /  1873 tokens\n",
      "2025-03-09 20:10:52,919 - INFO - Received question: What are the two expert load-balancing strategies in EPLB?\n",
      "2025-03-09 20:10:52,994 - INFO - Retrieved top-3 relevant chunks: [\"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\", '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ‚Äôs actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches ‚Äî meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1062 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "<answer>\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   42840.11 ms /  1062 tokens (   40.34 ms per token,    24.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59078.96 ms /   272 runs   (  217.20 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  103124.48 ms /  1334 tokens\n",
      "2025-03-09 20:12:36,152 - INFO - Received question: What architectural features make 3FS suitable for AI workloads?\n",
      "2025-03-09 20:12:36,224 - INFO - Retrieved top-3 relevant chunks: ['### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ‚Äôs actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches ‚Äî meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2√ó200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2√ó400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1√ó200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.']\n",
      "Llama.generate: 29 prefix-match hit, remaining 1130 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The context provided describes the EPLB (Expert Parallelism Load Balancer) and outlines two different load-balancing strategies for handling varying expert loads across GPUs. These strategies are based on the number of server nodes and the number of expert groups, and they are used to distribute the experts (in terms of replication and placement) to ensure load balancing across GPUs. The context mentions that there are two policies: one for hierarchical load balancing and one for global load balancing. This information directly answers the question about the two expert load-balancing strategies in EPLB.</reasoning>\n",
      "<answer>\n",
      "In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "</answer>  \n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   49799.38 ms /  1130 tokens (   44.07 ms per token,    22.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  123701.56 ms /   778 runs   (  159.00 ms per token,     6.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  176817.92 ms /  1908 tokens\n",
      "2025-03-09 20:15:33,063 - INFO - Received question: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "2025-03-09 20:15:33,123 - INFO - Retrieved top-3 relevant chunks: ['## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ‚Äôs actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches ‚Äî meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '# DualPipe\\nDualPipe is an innovative bidirectional pipeline parallelism algorithm introduced in the DeepSeek-V3 Technical Report. It achieves full overlap of forward and backward computation-communication phases, also reducing pipeline bubbles. For detailed information on computation-communication overlap, please refer to the profile data.\\n\\nPipeline Bubbles and Memory Usage Comparison\\n\\n| Method    | Bubble                  | Parameter | Activation |\\n|:---------:|:-----------------------:|:---------:|:----------:|\\n| 1F1B      | (PP-1)(ùêπ+ùêµ)            | 1√ó        | PP         |\\n| ZB1P      | (PP-1)(ùêπ+ùêµ-2ùëä)         | 1√ó        | PP         |\\n| DualPipe  | (PP/2-1)(ùêπ&ùêµ+ùêµ-3ùëä)     | 2√ó        | PP+1       |\\n\\nùêπ denotes the execution time of a forward chunk, ùêµ denotes the execution time of a full backward chunk, ùëä denotes the execution time of a \"backward for weights\" chunk, and ùêπ&ùêµ denotes the execution time of two mutually overlapped forward and backward chunks.\\n\\n### About\\nA bidirectional pipeline parallelism algorithm for computation-communication overlap in V3/R1 training\\n\\n`DualPipe was created and developed by Jiashi Li and Chengqi Deng and Wenfeng Liang.`\\n\\n# Profiling Data in DeepSeek Infra\\nHere, we publicly share profiling data from our training and inference framework to help the community better understand the communication-computation overlap strategies and low-level implementation details. The profiling data was captured using the PyTorch Profiler. After downloading, you can visualize it directly by navigating to chrome://tracing in the Chrome browser (or edge://tracing in the Edge browser). Notice that we simulate an absolutely balanced MoE routing strategy for profiling.', \"# Expert Parallelism Load Balancer (EPLB)\\n\\nWhen using expert parallelism (EP), different experts are assigned to different GPUs. Because the load of different experts may vary depending on the current workload, it is important to keep the load of different GPUs balanced. As described in the DeepSeek-V3 paper, we adopt a redundant experts strategy that duplicates heavy-loaded experts. Then, we heuristically pack the duplicated experts to GPUs to ensure load balancing across different GPUs. Moreover, thanks to the group-limited expert routing used in DeepSeek-V3, we also attempt to place the experts of the same group to the same node to reduce inter-node data traffic, whenever possible.\\n\\nTo facilitate reproduction and deployment, we open-source our deployed EP load balancing algorithm in eplb.py. The algorithm computes a balanced expert replication and placement plan based on the estimated expert loads. Note that the exact method to predict the loads of experts is out of this repo's scope. A common method is to use moving average of historical statistics.\\n\\n## The Algorithm\\n\\nThe load balancing algorithm comes with two policies used for different cases.\\n\\n## Hierarchical Load Balancing\\n\\nWhen the number of server nodes divides the number of expert groups, we use the hierarchical load balancing policy to harness the group-limited expert routing. We first pack the expert groups to nodes evenly, ensuring the loads of different nodes are balanced. Then, we replicate the experts within each node. Finally, we pack the replicated experts to individual GPUs to ensure different GPUs are load-balanced. The hierarchical load balancing policy can be used in prefilling stage with a smaller expert-parallel size.\\n\\n### Global Load Balancing\"]\n",
      "Llama.generate: 29 prefix-match hit, remaining 1090 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The Fire-Flyer File System (3FS) is designed to be highly suitable for AI workloads, particularly given the high-throughput and strong consistency requirements. Several architectural features of 3FS make it a good fit for AI workloads, including its:\n",
      "\n",
      "1. Disaggregated Architecture: The system combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resources in a locality-oblivious manner. This feature is particularly useful in AI workloads where data access patterns are highly irregular and can vary significantly across different nodes.\n",
      "\n",
      "2. Strong Consistency: The system implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency. This feature simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. File Interfaces: Developing stateless metadata services backed by a transactional key-value store (e.g., FoundationDB) makes the file interface well-known and used everywhere, reducing the learning curve for new users.\n",
      "\n",
      "4. Data Preparation: Organizing outputs of data analytics pipelines into hierarchical directory structures and managing large volumes of intermediate outputs efficiently, which is beneficial for AI workloads dealing with large datasets and complex data pipelines.\n",
      "\n",
      "5. Dataloaders: Eliminating the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes. This feature simplifies the process of accessing large datasets, which is common in AI workloads like training and inference.\n",
      "\n",
      "6. Checkpointing: Supporting high-throughput parallel checkpointing for large-scale training, which is essential for AI training workflows that often involve iterative training and checkpointing.\n",
      "\n",
      "7. KVCache for Inference: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity, which is crucial for optimizing the inference process in AI systems.\n",
      "\n",
      "These features collectively make 3FS a suitable architectural choice for AI workloads, especially in environments that require high-throughput, strong consistency, and efficient data management.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   34992.54 ms /  1090 tokens (   32.10 ms per token,    31.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26253.23 ms /   175 runs   (  150.02 ms per token,     6.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   61777.28 ms /  1265 tokens\n",
      "2025-03-09 20:16:34,915 - INFO - Received question: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "2025-03-09 20:16:34,992 - INFO - Retrieved top-3 relevant chunks: ['## Performance\\n1. Peak throughput\\n\\nThe following figure demonstrates the throughput of read stress test on a large 3FS cluster. This cluster consists of 180 storage nodes, each equipped with 2√ó200Gbps InfiniBand NICs and sixteen 14TiB NVMe SSDs. Approximately 500+ client nodes were used for the read stress test, with each client node configured with 1x200Gbps InfiniBand NIC. The final aggregate read throughput reached approximately 6.6 TiB/s with background traffic from training jobs.\\n\\n2. GraySort\\n\\nWe evaluated smallpond using the GraySort benchmark, which measures sort performance on large-scale datasets. Our implementation adopts a two-phase approach: (1) partitioning data via shuffle using the prefix bits of keys, and (2) in-partition sorting. Both phases read/write data from/to 3FS.\\n\\nThe test cluster comprised 25 storage nodes (2 NUMA domains/node, 1 storage service/NUMA, 2√ó400Gbps NICs/node) and 50 compute nodes (2 NUMA domains, 192 physical cores, 2.2 TiB RAM, and 1√ó200 Gbps NIC/node). Sorting 110.5 TiB of data across 8,192 partitions completed in 30 minutes and 14 seconds, achieving an average throughput of 3.66 TiB/min.\\n\\n3. KVCache\\n\\nKVCache is a technique used to optimize the LLM inference process. It avoids redundant computations by caching the key and value vectors of previous tokens in the decoder layers. The top figure demonstrates the read throughput of all KVCache clients, highlighting both peak and average values, with peak throughput reaching up to 40 GiB/s. The bottom figure presents the IOPS of removing ops from garbage collection (GC) during the same time period.', '## Training\\nThe training profile data demonstrates our overlapping strategy for a pair of individual forward and backward chunks in DualPipe. Each chunk contains 4 MoE (Mixture of Experts) layers. The parallel configuration aligns with DeepSeek-V3 pretraining settings: EP64, TP1 with 4K sequence length. And the PP communication is not included during profilng for simplicity.\\n\\n## Inference\\n### Prefilling\\nFor prefilling, the profile employs EP32 and TP1 (in line with DeepSeek V3/R1 ‚Äôs actual online deployment), with a prompt length set to 4K and a batch size of 16K tokens per GPU. In our prefilling stage, we utilize two micro-batches to overlap computation and all-to-all communication, while ensuring that the attention computation load is balanced across the two micro-batches ‚Äî meaning that the same prompt may be split between them.\\n\\n### Decoding\\nFor decoding, the profile employs EP128, TP1, and a prompt length of 4K (closely matching the actual online deployment configuration), with a batch size of 128 requests per GPU. Similar to prefilling, decoding also leverages two micro-batches for overlapping computation and all-to-all communication. However, unlike in prefilling, the all-to-all communication during decoding does not occupy GPU SMs: after RDMA messages are issued, all GPU SMs are freed, and the system waits for the all-to-all communication to complete after the computation has finished. For more information about the all-to-all implementation, please refer to DeepEP.\\n\\n# Expert Parallelism Load Balancer (EPLB)', '### Global Load Balancing\\n\\nIn other cases, we use the global load balancing policy that replicates the experts globally regardless of expert groups, and pack the replicated experts to individual GPUs. This policy can be adopted in decoding stage with a larger expert-parallel size.\\n\\n# Fire-Flyer File system\\nThe Fire-Flyer File System (3FS) is a high-performance distributed file system designed to address the challenges of AI training and inference workloads. It leverages modern SSDs and RDMA networks to provide a shared storage layer that simplifies development of distributed applications. Key features and benefits of 3FS include:\\n\\n- Performance and Usability\\n\\n    - Disaggregated Architecture Combines the throughput of thousands of SSDs and the network bandwidth of hundreds of storage nodes, enabling applications to access storage resource in a locality-oblivious manner.\\n    - Strong Consistency Implements Chain Replication with Apportioned Queries (CRAQ) for strong consistency, making application code simple and easy to reason about.\\n    - File Interfaces Develops stateless metadata services backed by a transactional key-value store (e.g., FoundationDB). The file interface is well known and used everywhere. There is no need to learn a new storage API.\\n\\n- Diverse Workloads\\n\\n    - Data Preparation Organizes outputs of data analytics pipelines into hierarchical directory structures and manages a large volume of intermediate outputs efficiently.\\n    - Dataloaders Eliminates the need for prefetching or shuffling datasets by enabling random access to training samples across compute nodes.\\n    - Checkpointing Supports high-throughput parallel checkpointing for large-scale training.\\n    - KVCache for Inference Provides a cost-effective alternative to DRAM-based caching, offering high throughput and significantly larger capacity.\\n\\n## Performance\\n1. Peak throughput']\n",
      "Llama.generate: 30 prefix-match hit, remaining 1128 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The given context does not provide any information about 3FS (which seems to be a typo or misunderstanding, as the context is about DeepSeek-V3 and DualPipe), nor does it mention KVCache optimization for LLM inference. There is no data or information related to 3FS or KVCache optimization in the provided context. Therefore, based on the information given, it's impossible to answer the question about 3FS and KVCache optimization.\n",
      "</reasoning>\n",
      "<answer>\n",
      "The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "</answer>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   77185.40 ms\n",
      "llama_perf_context_print: prompt eval time =   32910.38 ms /  1128 tokens (   29.18 ms per token,    34.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44383.15 ms /   300 runs   (  147.94 ms per token,     6.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   78335.24 ms /  1428 tokens\n",
      "2025-03-09 20:17:53,343 - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: <reasoning>\n",
      "The GraySort benchmark results demonstrate 3FS's capabilities by showcasing how efficiently it can handle large-scale data sorting tasks. In this case, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,19gestions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The ability to sort such a large dataset in a relatively short time demonstrates 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</reasoning>\n",
      "<answer>\n",
      "The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.</answer>\n",
      "Evaluating generated answers...\n",
      "\n",
      "Evaluation Results:\n",
      "Question 1: How does DualPipe optimize pipeline parallelism compared to 1F1B and ZB1P?\n",
      "Generated Answer: <reasoning>\n",
      "To compare DualPipe with 1F1B (forward-backward) and ZB1P (Zero-Backward-1-Partition), we need to focus on the bubble reduction and overall performance improvements. The \"Bubble\" column in the table details the performance improvement of each method, which is a key metric for evaluating pipeline parallelism efficiency.\n",
      "\n",
      "1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) are both forward-backward pipeline parallelism methods, but they differ in how they handle the communication phases. 1F1B (Forward-Backward) and ZB1P (Zero-Backward-1-Partition) have different numbers of pipeline bubbles. \n",
      "\n",
      "DualPipe introduces an innovation by using an overlap of 2 forward and backward chunks, which reduces the number of pipeline bubbles compared to the other methods. \n",
      "\n",
      "\n",
      "DualPipe optimizes pipeline parallelism by using an overlap of 2 forward and backward chunks, which results in fewer pipeline bubbles compared to 1F1B and ZB1P. This overlap allows for better utilization of the pipeline, reducing the number of idle cycles between forward and backward computation phases, and thereby improving overall performance and efficiency.\n",
      "Reference Answer: DualPipe reduces pipeline bubbles to $$(PP/2-1)(F\\&B + B - 3W)$$ using bidirectional parallelism, while 1F1B and ZB1P have bubbles of $$(PP-1)(F+B)$$ and $$(PP-1)(F+B-2W)$$ respectively. It uses 2√ó parameter memory and PP+1 activation memory, enabling full computation-communication overlap for forward/backward phases.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.16568047337278108, recall=0.5384615384615384, fmeasure=0.25339366515837103), 'rouge2': Score(precision=0.023809523809523808, recall=0.0784313725490196, fmeasure=0.0365296803652968), 'rougeL': Score(precision=0.11242603550295859, recall=0.36538461538461536, fmeasure=0.17194570135746606)}\n",
      "Question 2: What are the two expert load-balancing strategies in EPLB?\n",
      "Generated Answer: In EPLB, the two expert load-balancing strategies are:\n",
      "\n",
      "1. Hierarchical Load Balancing: This strategy is used when the number of server nodes divides the number of expert groups. It ensures that the loads of different nodes are balanced by first distributing the expert groups evenly across the nodes, then replicating the experts within each node, and finally packing the replicated experts to individual GPUs to ensure different GPUs are load-balanced.\n",
      "\n",
      "2. Global Load Balancing: This strategy is used in cases where the hierarchical load balancing is not applicable. It involves replicating the experts globally regardless of expert groups and packing them to individual GPUs. This policy is used in the decoding stage with a larger expert-parallel size.\n",
      "Reference Answer: EPLB employs: 1) **Hierarchical Load Balancing** (groups experts by nodes to minimize inter-node traffic, used when nodes divide expert groups evenly), and 2) **Global Load Balancing** (replicates experts across all GPUs regardless of groups, typically for decoding stages with larger EP sizes).\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.2916666666666667, recall=0.813953488372093, fmeasure=0.4294478527607362), 'rouge2': Score(precision=0.11764705882352941, recall=0.3333333333333333, fmeasure=0.1739130434782609), 'rougeL': Score(precision=0.225, recall=0.627906976744186, fmeasure=0.33128834355828224)}\n",
      "Question 3: What architectural features make 3FS suitable for AI workloads?\n",
      "Generated Answer: The Fire-Flyer File System (3FS) is designed with several architectural features that make it highly suitable for AI workloads. These features include:\n",
      "\n",
      "1. **Disaggregated Architecture**: This allows for efficient access to storage resources without being tied to the physical location of the data, which is beneficial for the irregular and varying data access patterns typical in AI workloads.\n",
      "\n",
      "2. **Strong Consistency**: Through the implementation of Chain Replication with Apportioned Queries (CRAQ), the system ensures strong data consistency. This simplifies application code and reasoning about consistency, which is crucial for AI systems that often require high levels of data integrity and consistency.\n",
      "\n",
      "3. **File Interfaces**: By developing stateless metadata services backed by a transactional key-value store, 3FS maintains a well-known and easily manageable interface for developers, reducing the learning curve and ensuring seamless integration.\n",
      "\n",
      "4. **Data Preparation**: Efficient organization of data analytics pipelines into hierarchical directory structures and management of large volumes of intermediate outputs simplify the workflow and ensure that data preparation is streamlined.\n",
      "\n",
      "5. **Dataloaders**: Enable random access to training samples across compute nodes without the need for prefetching or shuffling, making the data access process more efficient.\n",
      "\n",
      "6. **Checkpoints**: High-throughput parallel checkpointing for large-scale training is supported, which is essential for iterative training processes in AI systems.\n",
      "\n",
      "7. **KVCache for Inference**: Provides a cost-effective alternative to DRAM-based caching, offering high throughput and larger capacity, which optimizes the inference process.\n",
      "\n",
      "These features collectively enable 3FS to be a robust solution for AI workloads, addressing the demands for high-throughput, strong consistency, and efficient data management in AI systems.\n",
      "Reference Answer: 3FS combines disaggregated RDMA/SSD storage with CRAQ-based strong consistency. It provides standard file interfaces via FoundationDB-managed metadata, supports dataloaders/checkpointing/KVCache, and delivers 6.6 TiB/s aggregate read throughput in production clusters.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.08270676691729323, recall=0.6111111111111112, fmeasure=0.14569536423841062), 'rouge2': Score(precision=0.007547169811320755, recall=0.05714285714285714, fmeasure=0.013333333333333332), 'rougeL': Score(precision=0.06015037593984962, recall=0.4444444444444444, fmeasure=0.10596026490066225)}\n",
      "Question 4: How does 3FS perform in KVCache optimization for LLM inference?\n",
      "Generated Answer: The information provided does not contain any details about 3FS or its performance in KVCache optimization for Large Language Models (LLMs) during inference. Since 3FS isn't mentioned in the context, and there's no data or information about KVCache optimization, it's not possible to answer this question based on the given context.\n",
      "Reference Answer: 3FS achieves 40 GiB/s peak KVCache read throughput with efficient garbage collection (high IOPS removal operations). It serves as a cost-effective DRAM alternative, handling 6.6 TiB/s read throughput across 500+ client nodes in production.\n",
      "BLEU Score: 0.0000\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.1111111111111111, recall=0.15789473684210525, fmeasure=0.13043478260869565), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.09259259259259259, recall=0.13157894736842105, fmeasure=0.10869565217391304)}\n",
      "Question 5: What GraySort benchmark results highlight 3FS's capabilities?\n",
      "Generated Answer: The GraySort benchmark results highlight 3FS's capability by demonstrating that it can efficiently process and sort large-scale datasets. In this specific test, 3FS was able to achieve an average throughput of 3.66 TiB/min when sorting 110.5 TiB of data across 8,192 partitions in 30 minutes and 14 seconds. This performance indicates that 3FS is capable of handling large datasets and sorting them efficiently, which is a crucial aspect for distributed systems that need to process large amounts of data in a timely manner. The results showcase 3FS's strong performance and suitability for applications that require high throughput and strong consistency.\n",
      "Reference Answer: 3FS sorted 110.5 TiB of data in 30m14s using a two-phase shuffle-and-sort approach, achieving 3.66 TiB/min throughput across 25 storage/50 compute nodes. This demonstrates its large-scale data processing efficiency for AI workloads.\n",
      "BLEU Score: 0.0393\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.24299065420560748, recall=0.65, fmeasure=0.35374149659863946), 'rouge2': Score(precision=0.10377358490566038, recall=0.28205128205128205, fmeasure=0.15172413793103448), 'rougeL': Score(precision=0.12149532710280374, recall=0.325, fmeasure=0.17687074829931973)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python312\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import logging\n",
    "import os \n",
    "\n",
    "# Define the process_file function\n",
    "def process_file_from_path(file_path):\n",
    "    \"\"\"\n",
    "    Processes a file based on its extension (.pdf, .txt, .md) and indexes its content into a vector database.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        str: Success or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            return f\"‚ùå File not found: {file_path}\"\n",
    "\n",
    "        # Determine the file extension\n",
    "        file_extension = file_path.split('.')[-1].lower()\n",
    "\n",
    "        # Extract text based on file type\n",
    "        if file_extension == 'pdf':\n",
    "            # Process PDF files using PyMuPDF\n",
    "            doc = fitz.open(file_path)\n",
    "            extracted_text = \"\"\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                extracted_text += page.get_text()\n",
    "        elif file_extension in ['txt', 'md']:\n",
    "            # Process plain text or markdown files\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                extracted_text = f.read()\n",
    "        else:\n",
    "            return \"‚ùå Unsupported file type. Please upload a .pdf, .txt, or .md file.\"\n",
    "\n",
    "        # Clean non-UTF-8 characters from extracted text\n",
    "        cleaned_text = extracted_text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "        # Split cleaned text into chunks for indexing\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=50)\n",
    "        texts = splitter.split_text(cleaned_text)\n",
    "\n",
    "        # Index chunks into ColBERT or another vector database\n",
    "        global file_embeddings, file_text_chunks\n",
    "        file_embeddings.clear()\n",
    "        file_text_chunks.clear()\n",
    "\n",
    "        for chunk in texts:\n",
    "            inputs = colbert_tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = colbert_model(**inputs).last_hidden_state.mean(dim=1)  # Average pooling\n",
    "                file_embeddings.append(embedding.cpu())\n",
    "                file_text_chunks.append(chunk)\n",
    "\n",
    "        logging.info(f\"Processed and indexed {len(texts)} chunks from the uploaded {file_extension.upper()} file.\")\n",
    "        return f\"‚úÖ {file_extension.upper()} file processed successfully!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "        return f\"‚ùå Error processing file: {str(e)}\"\n",
    "\n",
    "# Load JSON Dataset\n",
    "def load_dataset(json_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a JSON file.\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: Loaded dataset as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated answer from the RAG system.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"‚ö†Ô∏è No relevant context found.\":\n",
    "        error_msg = \"‚ö†Ô∏è No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the full response text\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        print(\"Response:\", response_text)\n",
    "\n",
    "        # Parse reasoning and answer from the response\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return error_msg\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    Returns:\n",
    "        list: List of generated answers.\n",
    "    \"\"\"\n",
    "    generated_answers = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        generated_answer = rag_system(question, chat_history)\n",
    "        generated_answers.append(generated_answer)\n",
    "    \n",
    "    return generated_answers\n",
    "\n",
    "# Evaluate Generated Answers\n",
    "def evaluate_answers(generated_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using BLEU and ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        generated_answers (list): List of answers generated by the RAG system.\n",
    "        reference_answers (list): List of reference answers from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing BLEU and ROUGE scores for each answer pair.\n",
    "    \"\"\"\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    results = {\"bleu_scores\": [], \"rouge_scores\": []}\n",
    "\n",
    "    for gen_answer, ref_answer in zip(generated_answers, reference_answers):\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([ref_answer.split()], gen_answer.split())\n",
    "        results[\"bleu_scores\"].append(bleu_score)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge_scorer_obj.score(ref_answer, gen_answer)\n",
    "        results[\"rouge_scores\"].append(rouge_scores)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "    if file:\n",
    "        # Load dataset\n",
    "        dataset = load_dataset(json_path)\n",
    "\n",
    "        # Extract questions and reference answers\n",
    "        questions = [entry[\"question\"] for entry in dataset]\n",
    "        reference_answers = [entry[\"reference_answer\"] for entry in dataset]\n",
    "\n",
    "        # Generate answers using RAG system\n",
    "        print(\"Generating answers using the RAG system...\")\n",
    "        generated_answers = generate_answers(rag_system, dataset)\n",
    "\n",
    "        # Evaluate generated answers against reference answers\n",
    "        print(\"Evaluating generated answers...\")\n",
    "        evaluation_results = evaluate_answers(generated_answers, reference_answers)\n",
    "\n",
    "        # Print Results\n",
    "        print(\"\\nEvaluation Results:\")\n",
    "        \n",
    "        for i, (question, gen_answer, ref_answer) in enumerate(zip(questions, generated_answers, reference_answers)):\n",
    "            print(f\"Question {i+1}: {question}\")\n",
    "            print(f\"Generated Answer: {gen_answer}\")\n",
    "            print(f\"Reference Answer: {ref_answer}\")\n",
    "            print(f\"BLEU Score: {evaluation_results['bleu_scores'][i]:.4f}\")\n",
    "            print(f\"ROUGE Scores: {evaluation_results['rouge_scores'][i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core.language_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     Faithfulness,\n\u001b[0;32m      4\u001b[0m     AnswerRelevance,\n\u001b[0;32m      5\u001b[0m     ContextRelevance,\n\u001b[0;32m      6\u001b[0m     AnswerSimilarity,\n\u001b[0;32m      7\u001b[0m     FactualCorrectness\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_metrics\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamples\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingleTurnSample\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CacheInterface, DiskCacheBackend, cacher\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvaluationDataset, MultiTurnSample, SingleTurnSample\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrun_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\llms\\Lib\\site-packages\\ragas\\evaluation.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseCallbackHandler, BaseCallbackManager\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embeddings \u001b[38;5;28;01mas\u001b[39;00m LangchainEmbeddings\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLanguageModel \u001b[38;5;28;01mas\u001b[39;00m LangchainLLM\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mragas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_analytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_was_completed\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.language_models'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    AnswerRelevance,\n",
    "    ContextRelevance,\n",
    "    AnswerSimilarity,\n",
    "    FactualCorrectness\n",
    ")\n",
    "from ragas.evaluation import evaluate_metrics\n",
    "from ragas.samples import SingleTurnSample\n",
    "\n",
    "# Define RAG System Function\n",
    "def rag_system(question, chat_history):\n",
    "    \"\"\"\n",
    "    Retrieves context using the RAG retrieval mechanism and generates an answer using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user-provided question.\n",
    "        chat_history (list): Chat history for storing responses.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Generated answer and retrieved context.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Received question: {question}\")\n",
    "    context = get_context(question)  # Retrieve context using your retrieval mechanism\n",
    "\n",
    "    if not context or context == \"‚ö†Ô∏è No relevant context found.\":\n",
    "        error_msg = \"‚ö†Ô∏è No relevant context found.\"\n",
    "        logging.warning(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "<|user|>\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            stop=[\"<|user|>\", \"<|system|>\", \"<|assistant|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract reasoning and answer from response\n",
    "        response_text = output['choices'][0]['text'].strip()\n",
    "        reasoning_start = response_text.find(\"<reasoning>\")\n",
    "        reasoning_end = response_text.find(\"</reasoning>\")\n",
    "        answer_start = response_text.find(\"<answer>\")\n",
    "        answer_end = response_text.find(\"</answer>\")\n",
    "\n",
    "        reasoning = \"\"\n",
    "        answer = \"\"\n",
    "\n",
    "        if reasoning_start != -1 and reasoning_end != -1:\n",
    "            reasoning = response_text[reasoning_start + len(\"<reasoning>\"):reasoning_end].strip()\n",
    "\n",
    "            if answer_start != -1 and answer_end != -1:\n",
    "                answer = response_text[answer_start + len(\"<answer>\"):answer_end].strip()\n",
    "            else:\n",
    "                remaining_text = response_text[reasoning_end + len(\"</reasoning>\"):].strip()\n",
    "                remaining_text = remaining_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "                answer = remaining_text\n",
    "        else:\n",
    "            reasoning = \"Reasoning not explicitly provided.\"\n",
    "            answer = response_text.replace(\"<answer>\", \"\").replace(\"</answer>\", \"\").strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error generating response: {str(e)}\"\n",
    "        logging.error(error_msg)\n",
    "        chat_history.append((question, error_msg))\n",
    "        return {\"answer\": error_msg, \"context\": \"\"}\n",
    "\n",
    "    return {\"answer\": answer, \"context\": context}\n",
    "\n",
    "# Generate Answers Using RAG System\n",
    "def generate_answers(rag_system, dataset):\n",
    "    \"\"\"\n",
    "    Generate answers for each question in the dataset using the RAG system.\n",
    "    \n",
    "    Args:\n",
    "        rag_system (function): Function to generate answers using the RAG system.\n",
    "        dataset (list): List of dictionaries containing questions and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries containing generated answers and contexts.\n",
    "    \"\"\"\n",
    "    generated_data = []\n",
    "    chat_history = []\n",
    "    \n",
    "    for entry in dataset:\n",
    "        question = entry[\"question\"]\n",
    "        result = rag_system(question, chat_history)\n",
    "        \n",
    "        generated_data.append({\n",
    "            \"question\": question,\n",
    "            \"generated_answer\": result[\"answer\"],\n",
    "            \"retrieved_context\": result[\"context\"],\n",
    "            \"reference_answer\": entry.get(\"reference_answer\", \"\")\n",
    "        })\n",
    "    \n",
    "    return generated_data\n",
    "\n",
    "# Evaluate Answers Using RAGAS Metrics\n",
    "def evaluate_ragas(generated_data):\n",
    "    \"\"\"\n",
    "    Evaluate generated answers against reference answers using RAGAS metrics.\n",
    "    \n",
    "    Args:\n",
    "        generated_data (list): List of dictionaries containing questions, generated answers, contexts, and reference answers.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing scores for all RAGAS metrics.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for data in generated_data:\n",
    "        sample = SingleTurnSample(\n",
    "            question=data[\"question\"],\n",
    "            retrieved_context=data[\"retrieved_context\"],\n",
    "            ground_truth=data[\"reference_answer\"],\n",
    "            prediction=data[\"generated_answer\"]\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Define metrics to evaluate\n",
    "    metrics = {\n",
    "        \"Faithfulness\": Faithfulness(),\n",
    "        \"Answer Relevance\": AnswerRelevance(),\n",
    "        \"Context Relevance\": ContextRelevance(),\n",
    "        \"Answer Similarity\": AnswerSimilarity(),\n",
    "        \"Factual Correctness\": FactualCorrectness()\n",
    "    }\n",
    "\n",
    "    # Evaluate metrics for all samples\n",
    "    results = evaluate_metrics(samples, metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main Function for Evaluation\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to JSON file containing questions and reference answers\n",
    "    json_path = \"D:\\\\Competitions\\\\39\\\\gguf\\\\qa_datasetmd.json\"  # Replace with your file path\n",
    "    file = process_file_from_path(\"D:\\\\Competitions\\\\39\\\\gguf\\\\dataset.md\")\n",
    "\n",
    "    # Load dataset\n",
    "    with open(json_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    # Generate answers using RAG system\n",
    "    print(\"Generating answers using the RAG system...\")\n",
    "    generated_data = generate_answers(rag_system, dataset)\n",
    "\n",
    "    # Evaluate generated answers using RAGAS metrics\n",
    "    print(\"Evaluating generated answers...\")\n",
    "    evaluation_results = evaluate_ragas(generated_data)\n",
    "\n",
    "    # Print Results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    \n",
    "    for metric_name, score in evaluation_results.items():\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
